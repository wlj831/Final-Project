[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m William Jackson",
    "section": "",
    "text": "I’m a QA Operations Specialist in the Pharmaceutical Industry. I specialize in ensuring that pharmaceutical products are within the regulatory guidelines presented under government regulatory bodies such as the FDA and other international organizations."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Hi, I’m William Jackson",
    "section": "About Me",
    "text": "About Me\nI hold a Masters in Buiness Administration  from Cal Poly Pomona. Most of work experience and educational experience has been in biotechnology and pharamaceutical manufacturing, quality control, and quality assurance. I will be graduating with my graduate degree from Cal Poly Pomona in the Spring of 2025 with hopes of entering management."
  },
  {
    "objectID": "index.html#lets-connect",
    "href": "index.html#lets-connect",
    "title": "Hi, I’m William Jackson",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nYou can email me at williamlloydjackson@gmail.com or connect on LinkedIn.\n\nWelcome to my portfolio."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Résumé",
    "section": "",
    "text": "You can view a PDF version of my résumé here.\n\nObjective\nUtilize my knowledge obtained in my bachelor’s degree of biology to work in the pharmaceutical and biotechnology industries. My industry and educational experiences have been focused in the following areas: manufacturing, quality control/assurance, implementation of GMP regulated SOPs, environmental monitoring, chemical analysis, genetic isolation of microbial DNA, and industry experience in an FDA regulated environment. These skills will allow me to be an optimal asset to any pharmaceutical or biotechnology company.\n\n\nTechnical Skills\n\nBasic Programming Languages: R and Python\n\nWeb Technologies: HTML, CSS/SCSS, Quarto, CMS Platforms, Analytics Tools\n\n\n\nEducation\n\n\n\nDegree\nYear\nCollege / Department\nInstitution\n\n\n\n\nMBA\n2023 – 2025\nBusiness\nCal Poly Pomona\n\n\nBA Biology\n2012 – 2017\nBiological Sciences\nCSUN\n\n\n\n\n\nWork Experience\n\nQA Operations Specialist, Kite Pharma (Gilead Sciences)\nMay 2022 – Present\nResponsible for receiving and ensuring that apheresis material for cell culture manufacturing at the El Segundo site is within the GMP regulations. Additional roles include final shipment verification, review of packaging and manufacturing MPRs, generating final label material, and verifying final label material.\nSKILLS OBTAINED:\n• Handling of Apheresis Material\n• Review of Manufacturing and Shipment Documentation\n• GMP/GDP documentation\n• FDA regulated environment • Initiating and resolving deviations within the department\n• Use of Oracle EBS, GLIMS, Kite Konnect, and Veeva Vault\n• Excel, Word, and PowerPoint\n• Understanding of the operations of cell culture manufacturing\n• CAR T-cell therapeutics\n\n\n\nQA Specialist, Gilead Sciences\nMarch 2021 – May 2022\nAssists the Gilead La Verne and San Dimas sites with investigations into root causes for deviations in the following departments: Manufacturing, Quality Control, Packaging, Quality Assurance, and Engineering departments.\nSKILLS OBTAINED:\n• Managing Multiple Investigations\n• Interdepartmental Communication\n• Investigation of Root Cause\n• Assisting in Initiating CAPAs\nReport Writing\n• Conducting Interdepartmental Meetings\n• Understanding of Site Wide Operations\n• Thorough Knowledge of GMPs and GLPs\n• Use of SCADA, PCS, Maximo, GTrack, and Veeva Vault\n• Excel, Word, and PowerPoint • Conducting Interviews"
  },
  {
    "objectID": "marketing-analytics.html",
    "href": "marketing-analytics.html",
    "title": "Projects in GBA 5910",
    "section": "",
    "text": "Below are selected projects that highlight my experience applying Data wrangling and Data Visualization to real-world challenges. These projects reflect my ability to combine statistical tools, campaign data, and strategic thinking to inform decisions and drive results.\nWhether developing dashboards, analyzing data performance, or uncovering insights through analytics, each project is rooted in the use of R studio, python, and data visualization for actionable items."
  },
  {
    "objectID": "marketing-analytics.html#what-youll-find",
    "href": "marketing-analytics.html#what-youll-find",
    "title": "Projects in GBA 5910",
    "section": "What You’ll Find",
    "text": "What You’ll Find\n\nData Wrangling: Cleaning data sets or larger amount of data through python and R strudio functions.\nData Visualization: Managing large data sets to depict the information through graphical imagery."
  },
  {
    "objectID": "listings/05-social-media-engagement.html",
    "href": "listings/05-social-media-engagement.html",
    "title": "Social Media Engagement Analysis",
    "section": "",
    "text": "Overview\nThis analysis examined engagement metrics across multiple social media platforms to identify content trends, audience preferences, and posting strategies that drive performance.\n\n\n\n\nLearning Outcomes\n\nAggregate and clean social media data.\n\nIdentify high-performing content formats.\n\nUnderstand audience behavior and peak times.\n\nSupport social strategy with data-backed recommendations.\n\nVisualize results in an executive summary.\n\n\n\n\nKey Skills Gained\n\nSocial analytics\n\nPlatform metric APIs\n\nContent performance tracking\n\nAudience insights\n\nStrategic planning"
  },
  {
    "objectID": "resume/Resume.html",
    "href": "resume/Resume.html",
    "title": "Objective",
    "section": "",
    "text": "Objective\nUtilize my knowledge obtained in my bachelor’s degree of biology to work in the pharmaceutical and biotechnology industries. My industry and educational experiences have been focused in the following areas: manufacturing, quality control/assurance, implementation of GMP regulated SOPs, environmental monitoring, chemical analysis, genetic isolation of microbial DNA, and industry experience in an FDA regulated environment. These skills will allow me to be an optimal asset to any pharmaceutical or biotechnology company.\n\n\nTechnical Skills\n\nProgramming Languages: R and Python\nWeb Technologies: HTML, CSS/SCSS, Quarto\n\n\n\nEducation\n\n\n# A tibble: 2 × 5\n  what                     when                            with      where why  \n  &lt;chr&gt;                    &lt;chr&gt;                           &lt;chr&gt;     &lt;chr&gt; &lt;lis&gt;\n1 Master of Business Admin 2023 – Expected Graduation 2025 Business… Cal … &lt;chr&gt;\n2 Bachelor of Biology      2012 – 2017                     Biologic… CSUN  &lt;chr&gt;\n\n\n\n\nWork Experience\n\n\n# A tibble: 2 × 5\n  what                        when                  with            where why   \n  &lt;chr&gt;                       &lt;chr&gt;                 &lt;chr&gt;           &lt;chr&gt; &lt;list&gt;\n1 QA Operations Specialist II May 2022 – Present    Kite Pharmaceu… &lt;NA&gt;  &lt;list&gt;\n2 QA Specialist               March 2021 – May 2022 Gilead Sciences &lt;NA&gt;  &lt;list&gt;"
  },
  {
    "objectID": "listings/03-customer-segmentation-clustering.html",
    "href": "listings/03-customer-segmentation-clustering.html",
    "title": "Customer Segmentation Using Clustering",
    "section": "",
    "text": "Overview\nIn this analysis, customer data was segmented using clustering algorithms to identify meaningful groups. These segments informed targeting strategies and personalization efforts.\n\n\n\n\nLearning Outcomes\n\nClean and prepare customer-level data.\n\nApply clustering techniques to group similar customers.\n\nInterpret clusters to define marketing personas.\n\nRecommend tailored messaging strategies.\n\nSupport targeted campaign development.\n\n\n\n\nKey Skills Gained\n\nK-means clustering\n\nData preprocessing\n\nCustomer profiling\n\nMarket segmentation strategy\n\nR or Python for machine learning"
  },
  {
    "objectID": "listings/01-campaign-insights-dashboard.html",
    "href": "listings/01-campaign-insights-dashboard.html",
    "title": "Campaign Insights Dashboard",
    "section": "",
    "text": "Overview\nThis project involved building a dashboard to track and visualize marketing campaign performance. Metrics like click-through rate, conversion rate, and ROI were tracked across channels using R and Tableau.\n\n\n\n\nLearning Outcomes\n\nVisualize and monitor key marketing metrics.\n\nAnalyze performance trends across multiple campaigns.\n\nIdentify high- and low-performing segments.\n\nCommunicate insights through interactive visuals.\n\nDevelop data-informed recommendations.\n\n\n\n\nKey Skills Gained\n\nDashboard development\n\nCampaign analytics\n\nData wrangling in R\n\nTableau or R Shiny visualization\n\nPerformance measurement"
  },
  {
    "objectID": "listings/02-seo-benchmarking-analysis.html",
    "href": "listings/02-seo-benchmarking-analysis.html",
    "title": "SEO Benchmarking Analysis",
    "section": "",
    "text": "Overview\nThis project compared SEO performance across competitors, highlighting keyword gaps, backlink profiles, and content effectiveness. Insights were used to guide an improved content and SEO strategy.\n\n\n\n\nLearning Outcomes\n\nIdentify keyword opportunities through competitive research.\n\nTrack and compare domain authority and page rankings.\n\nRecommend on-page SEO improvements.\n\nDesign a data-driven SEO content strategy.\n\nVisualize competitor positioning.\n\n\n\n\nKey Skills Gained\n\nKeyword research\n\nWeb scraping & data cleaning\n\nSEO analysis tools\n\nData storytelling\n\nStrategy development"
  },
  {
    "objectID": "listings/04-funnel-attribution-model.html",
    "href": "listings/04-funnel-attribution-model.html",
    "title": "Marketing Funnel Attribution Model",
    "section": "",
    "text": "Overview\nThis project developed a marketing attribution model to assign credit to touchpoints throughout the conversion journey. The insights supported channel optimization and budget reallocation.\n\n\n\n\nLearning Outcomes\n\nTrack multi-touch customer journeys.\n\nModel attribution using first-touch, last-touch, and linear approaches.\n\nVisualize funnel performance.\n\nQuantify channel impact on conversion.\n\nGuide future campaign investment decisions.\n\n\n\n\nKey Skills Gained\n\nAttribution modeling\n\nFunnel analysis\n\nData visualization\n\nCampaign performance analysis\n\nR or Python analytics pipelines"
  },
  {
    "objectID": "strategic-marketing.html",
    "href": "strategic-marketing.html",
    "title": "Strategic Marketing",
    "section": "",
    "text": "Below are selected projects that showcase my work in strategic marketing, campaign development, and brand management. These examples highlight how I apply leadership, data-informed decision making, and creative direction to drive marketing success."
  },
  {
    "objectID": "strategic-marketing.html#project-title-1-strategic-campaign-planning",
    "href": "strategic-marketing.html#project-title-1-strategic-campaign-planning",
    "title": "Strategic Marketing",
    "section": "[Project Title 1: Strategic Campaign Planning]",
    "text": "[Project Title 1: Strategic Campaign Planning]\nIn this project, I led the development of a comprehensive marketing campaign for [product, service, or initiative]. From audience segmentation to messaging strategy, I managed the full lifecycle of the campaign, aligning it with business objectives and measurable KPIs.\nSkills Demonstrated:\n\nMarketing Strategy: Defined objectives and tactics for maximum impact.\nTeam Coordination: Oversaw collaboration across creative, data, and channel teams.\nPerformance Tracking: Implemented real-time metrics and post-campaign analysis.\nBrand Positioning: Ensured consistency across all touchpoints."
  },
  {
    "objectID": "strategic-marketing.html#project-title-2-brand-audit-positioning",
    "href": "strategic-marketing.html#project-title-2-brand-audit-positioning",
    "title": "Strategic Marketing",
    "section": "[Project Title 2: Brand Audit & Positioning]",
    "text": "[Project Title 2: Brand Audit & Positioning]\nThis initiative involved conducting a full brand audit for [organization or brand], evaluating its current market perception, competitive landscape, and customer engagement. I synthesized insights into a refreshed brand strategy and visual identity recommendations.\nSkills Demonstrated:\n\nBrand Analysis: Audited messaging, tone, and visual identity.\nCompetitive Benchmarking: Evaluated brand positioning within the market.\nStrategic Communication: Developed new value propositions and storytelling elements.\nStakeholder Presentation: Delivered recommendations to senior leadership."
  },
  {
    "objectID": "strategic-marketing.html#project-title-3-product-launch-go-to-market-plan",
    "href": "strategic-marketing.html#project-title-3-product-launch-go-to-market-plan",
    "title": "Strategic Marketing",
    "section": "[Project Title 3: Product Launch Go-to-Market Plan]",
    "text": "[Project Title 3: Product Launch Go-to-Market Plan]\nFor the release of [new product/service], I created a go-to-market (GTM) strategy from scratch. This included customer journey mapping, channel mix planning, and launch timeline coordination.\nSkills Demonstrated:\n\nGo-to-Market Strategy: Developed a clear launch roadmap aligned with buyer needs.\nIntegrated Marketing: Orchestrated email, paid ads, social, and on-site promotions.\nBudget Management: Allocated resources efficiently across tactics.\nLaunch Optimization: Tracked performance and refined tactics in real time.\n\nBelow is an embedded demonstration of the interactive report:"
  },
  {
    "objectID": "strategic-marketing.html#project-title-4-client-consulting-campaign-optimization",
    "href": "strategic-marketing.html#project-title-4-client-consulting-campaign-optimization",
    "title": "Strategic Marketing",
    "section": "[Project Title 4: Client Consulting & Campaign Optimization]",
    "text": "[Project Title 4: Client Consulting & Campaign Optimization]\nI collaborated with a client to assess and optimize their existing marketing efforts. Through audits, persona development, and campaign refinement, I helped improve ROI and audience engagement.\nSkills Demonstrated:\n\nClient Consulting: Provided clear insights and strategic guidance.\nPersona Development: Built target personas to guide messaging.\nCampaign Optimization: Identified and addressed areas of underperformance.\nData-Driven Adjustments: Leveraged analytics to refine tactics."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "",
    "text": "# load the tidyverse library\nlibrary(tidyverse)\n\n#convert the built-in \"airquality\" dataset to a tibble\n\nair.tib &lt;- as_tibble(airquality)\n\n#confirm the operation was successful \nprint(air.tib)\n\n# A tibble: 153 × 6\n   Ozone Solar.R  Wind  Temp Month   Day\n   &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    41     190   7.4    67     5     1\n 2    36     118   8      72     5     2\n 3    12     149  12.6    74     5     3\n 4    18     313  11.5    62     5     4\n 5    NA      NA  14.3    56     5     5\n 6    28      NA  14.9    66     5     6\n 7    23     299   8.6    65     5     7\n 8    19      99  13.8    59     5     8\n 9     8      19  20.1    61     5     9\n10    NA     194   8.6    69     5    10\n# ℹ 143 more rows\n\n# Convert air.tib back to data frame \nair.df &lt;- as.data.frame(air.tib)\n\n#confirm the operation was successful \nprint(air.df [1:20, ])\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n11     7      NA  6.9   74     5  11\n12    16     256  9.7   69     5  12\n13    11     290  9.2   66     5  13\n14    14     274 10.9   68     5  14\n15    18      65 13.2   58     5  15\n16    14     334 11.5   64     5  16\n17    34     307 12.0   66     5  17\n18     6      78 18.4   57     5  18\n19    30     322 11.5   68     5  19\n20    11      44  9.7   62     5  20\n\n\n\n\n# Base R Approach \nair.tib[air.tib$Temp == 97, ]\n\n# A tibble: 1 × 6\n  Ozone Solar.R  Wind  Temp Month   Day\n  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    76     203   9.7    97     8    28\n\n# Extract Only the Ozone Value \nair.tib$Ozone[air.tib$Temp == 97]\n\n[1] 76\n\n\n\n# Tidyverse Approach \n\n#load dplyr library \nlibrary(dplyr)\n\n#Subset the Entire Row Where Temp == 97\n\nair.tib %&gt;% filter(Temp == 97)\n\n# A tibble: 1 × 6\n  Ozone Solar.R  Wind  Temp Month   Day\n  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    76     203   9.7    97     8    28\n\n# Extract only the ozone value \n\nair.tib %&gt;% filter(Temp == 97) %&gt;% pull(Ozone)\n\n[1] 76"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-using-the-tidyverse-library-set-the-built-in-airquality-dataset-to-a-tibble-format-named-air.tib.-confirm-that-the-operation-was-successful.-next-create-a-new-data-frame-from-air.tib-and-assign-air.df-to-the-data-frame.-confirm-that-the-operation-was-successful.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-using-the-tidyverse-library-set-the-built-in-airquality-dataset-to-a-tibble-format-named-air.tib.-confirm-that-the-operation-was-successful.-next-create-a-new-data-frame-from-air.tib-and-assign-air.df-to-the-data-frame.-confirm-that-the-operation-was-successful.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "",
    "text": "# load the tidyverse library\nlibrary(tidyverse)\n\n#convert the built-in \"airquality\" dataset to a tibble\n\nair.tib &lt;- as_tibble(airquality)\n\n#confirm the operation was successful \nprint(air.tib)\n\n# A tibble: 153 × 6\n   Ozone Solar.R  Wind  Temp Month   Day\n   &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    41     190   7.4    67     5     1\n 2    36     118   8      72     5     2\n 3    12     149  12.6    74     5     3\n 4    18     313  11.5    62     5     4\n 5    NA      NA  14.3    56     5     5\n 6    28      NA  14.9    66     5     6\n 7    23     299   8.6    65     5     7\n 8    19      99  13.8    59     5     8\n 9     8      19  20.1    61     5     9\n10    NA     194   8.6    69     5    10\n# ℹ 143 more rows\n\n# Convert air.tib back to data frame \nair.df &lt;- as.data.frame(air.tib)\n\n#confirm the operation was successful \nprint(air.df [1:20, ])\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n11     7      NA  6.9   74     5  11\n12    16     256  9.7   69     5  12\n13    11     290  9.2   66     5  13\n14    14     274 10.9   68     5  14\n15    18      65 13.2   58     5  15\n16    14     334 11.5   64     5  16\n17    34     307 12.0   66     5  17\n18     6      78 18.4   57     5  18\n19    30     322 11.5   68     5  19\n20    11      44  9.7   62     5  20"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#using-the-tibble-you-created-earlier-find-the-ozone-amount-when-the-temperature-was-97.-show-this-by-both-subsetting-the-entire-row-and-pinpointing-the-ozone-element-only.-also-for-both-ways-use-base-r-approach-as-well-as-tidyverse-approach.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#using-the-tibble-you-created-earlier-find-the-ozone-amount-when-the-temperature-was-97.-show-this-by-both-subsetting-the-entire-row-and-pinpointing-the-ozone-element-only.-also-for-both-ways-use-base-r-approach-as-well-as-tidyverse-approach.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "",
    "text": "# Base R Approach \nair.tib[air.tib$Temp == 97, ]\n\n# A tibble: 1 × 6\n  Ozone Solar.R  Wind  Temp Month   Day\n  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    76     203   9.7    97     8    28\n\n# Extract Only the Ozone Value \nair.tib$Ozone[air.tib$Temp == 97]\n\n[1] 76\n\n\n\n# Tidyverse Approach \n\n#load dplyr library \nlibrary(dplyr)\n\n#Subset the Entire Row Where Temp == 97\n\nair.tib %&gt;% filter(Temp == 97)\n\n# A tibble: 1 × 6\n  Ozone Solar.R  Wind  Temp Month   Day\n  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    76     203   9.7    97     8    28\n\n# Extract only the ozone value \n\nair.tib %&gt;% filter(Temp == 97) %&gt;% pull(Ozone)\n\n[1] 76"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#select-only-the-top-10-worst-arrival-delays-for-each-day-for-each-month-for-each-year.-using-the-subset-of-the-data-calculate-the-average-delay-for-each-month-regardless-of-the-dates-or-years.-hints-you-will-want-to-group-the-data-by-year-month-and-day.-then-you-will-want-to-ungroup-the-data-to-remove-the-groupings-and-do-another-grouping-to-calculate-means-by-month.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#select-only-the-top-10-worst-arrival-delays-for-each-day-for-each-month-for-each-year.-using-the-subset-of-the-data-calculate-the-average-delay-for-each-month-regardless-of-the-dates-or-years.-hints-you-will-want-to-group-the-data-by-year-month-and-day.-then-you-will-want-to-ungroup-the-data-to-remove-the-groupings-and-do-another-grouping-to-calculate-means-by-month.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n2.1 Select only the top 10 worst arrival delays for each day for each month for each year. Using the subset of the data, calculate the average delay for each month regardless of the dates or years. Hints: you will want to group the data by year, month, and day. Then, you will want to ungroup the data to remove the groupings and do another grouping to calculate means by month.",
    "text": "2.1 Select only the top 10 worst arrival delays for each day for each month for each year. Using the subset of the data, calculate the average delay for each month regardless of the dates or years. Hints: you will want to group the data by year, month, and day. Then, you will want to ungroup the data to remove the groupings and do another grouping to calculate means by month.\n\n#load the library nycflights and library dplyr\nlibrary(nycflights13)\nlibrary(dplyr)\n\n# load the flights dataset \ndata(\"flights\")\n\n\n# Select the top 10 worst arrival delays for each day \n\ntop_delays &lt;- flights %&gt;%\n  group_by(year, month, day) %&gt;% #group by year month and day \n  slice_max(arr_delay, n=10, with_ties = FALSE) %&gt;% # select the top 10 worst days \n  ungroup() #remove group for next step\n\n\navg_monthly_delay&lt;- top_delays %&gt;%\n  group_by(month) %&gt;% # group by month only\n  summarise(avg_arr_delay = mean(arr_delay, na.rm = TRUE)) #Calculate Avg \n\n# Print the results in minutes\nprint(avg_monthly_delay)\n\n# A tibble: 12 × 2\n   month avg_arr_delay\n   &lt;int&gt;         &lt;dbl&gt;\n 1     1          200.\n 2     2          199.\n 3     3          202.\n 4     4          220.\n 5     5          202.\n 6     6          249.\n 7     7          254.\n 8     8          195.\n 9     9          166.\n10    10          161.\n11    11          162.\n12    12          225."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#visualize-the-data-that-shows-average-delays-each-month.-style-the-chart-appropriately-as-you-learned-in-the-previous-modules-to-make-the-chart-presentable.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#visualize-the-data-that-shows-average-delays-each-month.-style-the-chart-appropriately-as-you-learned-in-the-previous-modules-to-make-the-chart-presentable.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n2.2 Visualize the data that shows average delays each month. Style the chart appropriately, as you learned in the previous modules, to make the chart presentable.",
    "text": "2.2 Visualize the data that shows average delays each month. Style the chart appropriately, as you learned in the previous modules, to make the chart presentable.\n\nggplot(avg_monthly_delay, aes(x = factor(month), y = avg_arr_delay, fill = factor(month))) +\n  geom_col(show.legend = FALSE) +  \n  labs(\n    title = \"Average Arrival Delay per Month (Top 10 Worst Delays Each Day)\",\n    x = \"Month\",\n    y = \"Average Arrival Delay (minutes)\",\n    caption = \"Data Source: nycflights13\"\n  ) +\n  theme_minimal() +  \n  scale_fill_viridis_d(option = \"inferno\") +  # Better color scale for &gt;9 categories\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n    axis.text.x = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#describe-what-you-learned-from-the-data-about-delays.-since-this-is-an-important-insight-you-will-want-the-audience-to-pay-attention-and-style-the-description-of-insights-using-appropriate-quarto-document-features.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#describe-what-you-learned-from-the-data-about-delays.-since-this-is-an-important-insight-you-will-want-the-audience-to-pay-attention-and-style-the-description-of-insights-using-appropriate-quarto-document-features.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n2.3 Describe what you learned from the data about delays. Since this is an important insight, you will want the audience to pay attention and style the description of insights using appropriate Quarto document features.",
    "text": "2.3 Describe what you learned from the data about delays. Since this is an important insight, you will want the audience to pay attention and style the description of insights using appropriate Quarto document features.\n\n\n\n\n\n\nResponse\n\n\n\n\n\nBased upon the presentation of the data, it appears that the highest months for travel include June, July, and December. This inherently makes sense as these are the most common months to travel for both families and individuals. Most of summer months are common times in which families or individuals will go on their annual vacations, so we see higher delays during these months to more travelers. In December, we see higher delays due to the fact that people are traveling to family for Christmas and New Years. The presentation of the bar graph is simple and gives insight to the most commonly experienced months in which a delay was experienced."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#take-a-look-at-the-built-in-data-frame-relig_income-from-tidyr-package-which-is-one-of-the-packages-included-in-the-mega-package-called-the-tidyverse-package.-since-you-already-loaded-it-up-with-the-package-earlier-the-data-set-should-show-up-when-you-type-it.-perform-some-built-in-functions-that-will-help-you-understand-the-data-type-of-data-size-missing-value-variables-etc.-what-can-you-tell-about-the-data-set",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#take-a-look-at-the-built-in-data-frame-relig_income-from-tidyr-package-which-is-one-of-the-packages-included-in-the-mega-package-called-the-tidyverse-package.-since-you-already-loaded-it-up-with-the-package-earlier-the-data-set-should-show-up-when-you-type-it.-perform-some-built-in-functions-that-will-help-you-understand-the-data-type-of-data-size-missing-value-variables-etc.-what-can-you-tell-about-the-data-set",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n3.1 Take a look at the built-in data frame “relig_income” from tidyr package, which is one of the packages included in the mega package called the Tidyverse package. Since you already loaded it up with the package earlier, the data set should show up when you type it. Perform some built-in functions that will help you understand the data – type of data, size, missing value, variables, etc. What can you tell about the data set?",
    "text": "3.1 Take a look at the built-in data frame “relig_income” from tidyr package, which is one of the packages included in the mega package called the Tidyverse package. Since you already loaded it up with the package earlier, the data set should show up when you type it. Perform some built-in functions that will help you understand the data – type of data, size, missing value, variables, etc. What can you tell about the data set?\n\n# Check the dataset \nhead(relig_income)\n\n# A tibble: 6 × 11\n  religion  `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic       27        34        60        81        76       137        122\n2 Atheist        12        27        37        52        35        70         73\n3 Buddhist       27        21        30        34        33        58         62\n4 Catholic      418       617       732       670       638      1116        949\n5 Don’t kn…      15        14        15        11        10        35         21\n6 Evangeli…     575       869      1064       982       881      1486        949\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n# check the structure \nstr(relig_income)\n\ntibble [18 × 11] (S3: tbl_df/tbl/data.frame)\n $ religion          : chr [1:18] \"Agnostic\" \"Atheist\" \"Buddhist\" \"Catholic\" ...\n $ &lt;$10k             : num [1:18] 27 12 27 418 15 575 1 228 20 19 ...\n $ $10-20k           : num [1:18] 34 27 21 617 14 869 9 244 27 19 ...\n $ $20-30k           : num [1:18] 60 37 30 732 15 ...\n $ $30-40k           : num [1:18] 81 52 34 670 11 982 9 238 24 25 ...\n $ $40-50k           : num [1:18] 76 35 33 638 10 881 11 197 21 30 ...\n $ $50-75k           : num [1:18] 137 70 58 1116 35 ...\n $ $75-100k          : num [1:18] 122 73 62 949 21 949 47 131 15 69 ...\n $ $100-150k         : num [1:18] 109 59 39 792 17 723 48 81 11 87 ...\n $ &gt;150k             : num [1:18] 84 74 53 633 18 414 54 78 6 151 ...\n $ Don't know/refused: num [1:18] 96 76 54 1489 116 ...\n\n# summary of data \nsummary(relig_income)\n\n   religion             &lt;$10k           $10-20k          $20-30k      \n Length:18          Min.   :  1.00   Min.   :  2.00   Min.   :   3.0  \n Class :character   1st Qu.: 12.25   1st Qu.: 14.75   1st Qu.:  17.0  \n Mode  :character   Median : 20.00   Median : 27.00   Median :  33.5  \n                    Mean   :107.22   Mean   :154.50   Mean   : 186.5  \n                    3rd Qu.:170.00   3rd Qu.:193.00   3rd Qu.: 192.0  \n                    Max.   :575.00   Max.   :869.00   Max.   :1064.0  \n    $30-40k          $40-50k         $50-75k           $75-100k     \n Min.   :  4.00   Min.   :  2.0   Min.   :   7.00   Min.   :  3.00  \n 1st Qu.: 15.75   1st Qu.: 15.0   1st Qu.:  34.25   1st Qu.: 25.25  \n Median : 40.00   Median : 34.0   Median :  66.50   Median : 65.50  \n Mean   :183.44   Mean   :171.4   Mean   : 288.06   Mean   :221.67  \n 3rd Qu.:198.75   3rd Qu.:166.8   3rd Qu.: 201.50   3rd Qu.:128.75  \n Max.   :982.00   Max.   :881.0   Max.   :1486.00   Max.   :949.00  \n   $100-150k         &gt;150k        Don't know/refused\n Min.   :  4.0   Min.   :  4.00   Min.   :   8.00   \n 1st Qu.: 22.5   1st Qu.: 23.75   1st Qu.:  41.25   \n Median : 48.5   Median : 53.50   Median :  74.50   \n Mean   :177.6   Mean   :144.89   Mean   : 340.06   \n 3rd Qu.:103.5   3rd Qu.:134.25   3rd Qu.: 294.75   \n Max.   :792.0   Max.   :634.00   Max.   :1529.00   \n\n# missing values \nsum(is.na(relig_income))\n\n[1] 0\n\n# dimensions of dataset \ndim(relig_income)\n\n[1] 18 11\n\n# check column names \ncolnames(relig_income)\n\n [1] \"religion\"           \"&lt;$10k\"              \"$10-20k\"           \n [4] \"$20-30k\"            \"$30-40k\"            \"$40-50k\"           \n [7] \"$50-75k\"            \"$75-100k\"           \"$100-150k\"         \n[10] \"&gt;150k\"              \"Don't know/refused\"\n\n# view the table \nprint(relig_income)\n\n# A tibble: 18 × 11\n   religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Agnostic      27        34        60        81        76       137        122\n 2 Atheist       12        27        37        52        35        70         73\n 3 Buddhist      27        21        30        34        33        58         62\n 4 Catholic     418       617       732       670       638      1116        949\n 5 Don’t k…      15        14        15        11        10        35         21\n 6 Evangel…     575       869      1064       982       881      1486        949\n 7 Hindu          1         9         7         9        11        34         47\n 8 Histori…     228       244       236       238       197       223        131\n 9 Jehovah…      20        27        24        24        21        30         15\n10 Jewish        19        19        25        25        30        95         69\n11 Mainlin…     289       495       619       655       651      1107        939\n12 Mormon        29        40        48        51        56       112         85\n13 Muslim         6         7         9        10         9        23         16\n14 Orthodox      13        17        23        32        32        47         38\n15 Other C…       9         7        11        13        13        14         18\n16 Other F…      20        33        40        46        49        63         46\n17 Other W…       5         2         3         4         2         7          3\n18 Unaffil…     217       299       374       365       341       528        407\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#it-is-a-tibble-data-format-with-18-rows-and-11-variables.-to-learn-more-about-this-data-look-it-up-in-help.-type-the-code-to-do-so.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#it-is-a-tibble-data-format-with-18-rows-and-11-variables.-to-learn-more-about-this-data-look-it-up-in-help.-type-the-code-to-do-so.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n3.2 It is a tibble data format with 18 rows and 11 variables. To learn more about this data, look it up in help. Type the code to do so.",
    "text": "3.2 It is a tibble data format with 18 rows and 11 variables. To learn more about this data, look it up in help. Type the code to do so.\n\nhelp(\"relig_income\")\n\n\nHint: use help() function. What does it tell you about the data and variables? Make sure you type your responses outside below the cord chunk.\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe function provides documentation on the data set, explaining what it represents, the source of the data, and its structure. The variables will also be present in the data set and will include the names, types and meanings. With usage the file can help show you examples of how to use the data in analysis or how to access variables within the data set."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-go-back-to-the-data.-think-about-the-relationships-among-the-variables-you-want-to-visualize.-one-potential-idea-is-to-show-what-the-frequency-count-of-income-brackets-look-like-by-the-religion-people-practice.-does-the-current-data-allow-you-to-support-your-goal-why-or-why-not.-if-the-data-doesnt-support-the-goal-how-should-the-data-be-reshaped-to-support-the-goal",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-go-back-to-the-data.-think-about-the-relationships-among-the-variables-you-want-to-visualize.-one-potential-idea-is-to-show-what-the-frequency-count-of-income-brackets-look-like-by-the-religion-people-practice.-does-the-current-data-allow-you-to-support-your-goal-why-or-why-not.-if-the-data-doesnt-support-the-goal-how-should-the-data-be-reshaped-to-support-the-goal",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n3.3 Now, go back to the data. Think about the relationships among the variables you want to visualize. One potential idea is to show what the frequency (count) of income brackets look like by the religion people practice. Does the current data allow you to support your goal? Why or why not. If the data doesn’t support the goal, how should the data be reshaped to support the goal?",
    "text": "3.3 Now, go back to the data. Think about the relationships among the variables you want to visualize. One potential idea is to show what the frequency (count) of income brackets look like by the religion people practice. Does the current data allow you to support your goal? Why or why not. If the data doesn’t support the goal, how should the data be reshaped to support the goal?\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Reshape the data to long format\nrelig_income_long &lt;- relig_income %&gt;%\n  pivot_longer(cols = -religion, names_to = \"income_bracket\", values_to = \"count\")\n\n\nHint: In your response, consider the conditions for tidy data and if the current data is tidy. Data is tidy when (1) observations in the rows are unique, (2) variables in the columns are consistent and unique so that we can perform statistics that are interpretable."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#reshape-the-data.-further-draw-a-plot-that-shows-income-distribution-by-religion.-do-all-the-necessary-operations-to-make-the-chart-visualize-the-tidied-data-well.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#reshape-the-data.-further-draw-a-plot-that-shows-income-distribution-by-religion.-do-all-the-necessary-operations-to-make-the-chart-visualize-the-tidied-data-well.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n3.4 Reshape the data. Further, draw a plot that shows income distribution by religion. Do all the necessary operations to make the chart visualize the tidied data well.",
    "text": "3.4 Reshape the data. Further, draw a plot that shows income distribution by religion. Do all the necessary operations to make the chart visualize the tidied data well.\n\n# Create the plot\nggplot(relig_income_long, aes(x = income_bracket, y = count, fill = religion)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Income Bracket Distribution by Religion\",\n       x = \"Income Bracket\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHint: Successful data should have four columns: “religion”, “income”, and “count”. Assign “relig_income_longer” to the tydy data and print it out. The column “Don’t know/refused” can be deleted if you wish."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#next-redo-3.4-using-one-long-chain-of-codes-from-the-first-step-to-the-last-that-spans-the-whole-data-wrangling-and-visualization.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#next-redo-3.4-using-one-long-chain-of-codes-from-the-first-step-to-the-last-that-spans-the-whole-data-wrangling-and-visualization.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n3.5 Next, redo 3.4 using one long chain of codes from the first step to the last that spans the whole data wrangling and visualization.",
    "text": "3.5 Next, redo 3.4 using one long chain of codes from the first step to the last that spans the whole data wrangling and visualization.\n\nlibrary(tidyverse)\n\n# Perform the entire data wrangling and visualization in one long chain\nggplot(relig_income %&gt;%\n         pivot_longer(cols = -religion, names_to = \"income_bracket\", values_to = \"count\"), \n       aes(x = income_bracket, y = count, fill = religion)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +  # Stacked bar chart\n  labs(title = \"Income Distribution by Religion\",\n       x = \"Income Bracket\",\n       y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#take-a-look-at-the-built-in-data-frame-us_rent_income-from-tidyr-package-which-is-one-of-the-packages-included-in-the-mega-package-called-tidyverse-package.-since-you-already-loaded-it-up-with-the-package-earlier-the-data-set-should-show-up-when-you-type-it.-perform-some-built-in-functions-that-will-help-you-understand-the-data-type-of-data-size-missing-value-variables-etc.-what-can-you-tell-about-the-data-set",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#take-a-look-at-the-built-in-data-frame-us_rent_income-from-tidyr-package-which-is-one-of-the-packages-included-in-the-mega-package-called-tidyverse-package.-since-you-already-loaded-it-up-with-the-package-earlier-the-data-set-should-show-up-when-you-type-it.-perform-some-built-in-functions-that-will-help-you-understand-the-data-type-of-data-size-missing-value-variables-etc.-what-can-you-tell-about-the-data-set",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n4.1 Take a look at the built-in data frame “us_rent_income” from tidyr package, which is one of the packages included in the mega package called Tidyverse package. Since you already loaded it up with the package earlier, the data set should show up when you type it. Perform some built-in functions that will help you understand the data – type of data, size, missing value, variables, etc. What can you tell about the data set?",
    "text": "4.1 Take a look at the built-in data frame “us_rent_income” from tidyr package, which is one of the packages included in the mega package called Tidyverse package. Since you already loaded it up with the package earlier, the data set should show up when you type it. Perform some built-in functions that will help you understand the data – type of data, size, missing value, variables, etc. What can you tell about the data set?\n\n# View the first few rows of the data\nhead(us_rent_income)\n\n# A tibble: 6 × 5\n  GEOID NAME    variable estimate   moe\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 01    Alabama income      24476   136\n2 01    Alabama rent          747     3\n3 02    Alaska  income      32940   508\n4 02    Alaska  rent         1200    13\n5 04    Arizona income      27517   148\n6 04    Arizona rent          972     4\n\n# Check the structure of the data (types of data in each column)\nstr(us_rent_income)\n\ntibble [104 × 5] (S3: tbl_df/tbl/data.frame)\n $ GEOID   : chr [1:104] \"01\" \"01\" \"02\" \"02\" ...\n $ NAME    : chr [1:104] \"Alabama\" \"Alabama\" \"Alaska\" \"Alaska\" ...\n $ variable: chr [1:104] \"income\" \"rent\" \"income\" \"rent\" ...\n $ estimate: num [1:104] 24476 747 32940 1200 27517 ...\n $ moe     : num [1:104] 136 3 508 13 148 4 165 5 109 3 ...\n\n# Get the dimensions of the data (rows and columns)\ndim(us_rent_income)\n\n[1] 104   5\n\n# Get summary statistics of the dataset\nsummary(us_rent_income)\n\n    GEOID               NAME             variable            estimate      \n Length:104         Length:104         Length:104         Min.   :  464.0  \n Class :character   Class :character   Class :character   1st Qu.:  864.5  \n Mode  :character   Mode  :character   Mode  :character   Median : 1507.0  \n                                                          Mean   :14923.1  \n                                                          3rd Qu.:28872.0  \n                                                          Max.   :43198.0  \n                                                          NA's   :1        \n      moe       \n Min.   :  2.0  \n 1st Qu.:  4.5  \n Median : 18.0  \n Mean   : 95.5  \n 3rd Qu.:153.5  \n Max.   :681.0  \n NA's   :1      \n\n# Check for missing values\nsum(is.na(us_rent_income))\n\n[1] 2\n\n#Entire Dataset \nprint(us_rent_income)\n\n# A tibble: 104 × 5\n   GEOID NAME       variable estimate   moe\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama    income      24476   136\n 2 01    Alabama    rent          747     3\n 3 02    Alaska     income      32940   508\n 4 02    Alaska     rent         1200    13\n 5 04    Arizona    income      27517   148\n 6 04    Arizona    rent          972     4\n 7 05    Arkansas   income      23789   165\n 8 05    Arkansas   rent          709     5\n 9 06    California income      29454   109\n10 06    California rent         1358     3\n# ℹ 94 more rows\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe data set is rent and income statistics in the US. The summary range gives me an overview of all of the data provided from the online source. Print function allows me to see the entirity of the data so that all data can be reviewed. The sum is.na function allowed me to skim the data to finding missing variables. Based upon this review, there are 2 missing variables throughout the data set. I will either delete the missing data or take the averages.\n\n\n\n4.2. You will see five variables: GEOID, NAME, variable, estimate, and moe. To understand the data, type a code that will show you the description of the data in the help. Then read the description of the variables. What does it tell you about the data and variables? What does each variable mean? Make sure you type your responses outside below the cord chunk.\n\n#help function to review source and information that is in the data set \nhelp(us_rent_income)\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe help function allows for me to review the data in more conceptual aspect. It gives me the source of that (how it was collected), the variables measured in the data, and the index that it was collected in the tidyr package. The variables are as follows: GEOID - the unique identifier for geographic region, NAME - the name of the geographic region (state name), Variable - the variable income such as yearly income and monthly rent, estimate - the actual or estimated value of interest, and moe - margin of error, which is the uncertainty in the estimate (90%)."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-go-back-to-the-data.-think-about-the-relationships-among-the-variables-you-want-to-visualize.-one-potential-idea-is-to-show-the-relationship-between-income-and-rent-across-the-states.-ask-yourself-if-the-current-data-shape-supports-your-visualization-goal.-why-or-why-not-if-the-data-doesnt-support-the-goal-how-should-the-data-be-reshaped-to-support-the-goal",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#now-go-back-to-the-data.-think-about-the-relationships-among-the-variables-you-want-to-visualize.-one-potential-idea-is-to-show-the-relationship-between-income-and-rent-across-the-states.-ask-yourself-if-the-current-data-shape-supports-your-visualization-goal.-why-or-why-not-if-the-data-doesnt-support-the-goal-how-should-the-data-be-reshaped-to-support-the-goal",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n4.2 Now, go back to the data. Think about the relationships among the variables you want to visualize. One potential idea is to show the relationship between income and rent across the states. Ask yourself if the current data shape supports your visualization goal. Why or why not? If the data doesn’t support the goal, how should the data be reshaped to support the goal?",
    "text": "4.2 Now, go back to the data. Think about the relationships among the variables you want to visualize. One potential idea is to show the relationship between income and rent across the states. Ask yourself if the current data shape supports your visualization goal. Why or why not? If the data doesn’t support the goal, how should the data be reshaped to support the goal?\n\nHints: In your response, consider the conditions for tidy data and if the current data is tidy. Data is tidy when (1) observations in the rows are unique, (2) variables in the columns are consistent and unique so that we can perform statistics that are interpretable.\n\n\n# Reshape the data so that 'variable' values become separate columns\nus_rent_income_tidy &lt;- us_rent_income %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\n# View the transformed dataset\nhead(us_rent_income_tidy)\n\n# A tibble: 6 × 5\n  GEOID NAME      moe income  rent\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 01    Alabama   136  24476    NA\n2 01    Alabama     3     NA   747\n3 02    Alaska    508  32940    NA\n4 02    Alaska     13     NA  1200\n5 04    Arizona   148  27517    NA\n6 04    Arizona     4     NA   972\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe data set is now more properly organized to look at the relationship between the location, income, and rent. This structure will allow for easy of visualization between the annual income and the rent associated with geographic regions. This data can now be presented in a scatter plot to give a better understanding of the relationship between income and rent.\n\n\n\n4.4. Reshape the data. Further, draw a plot that shows the relationship between the income and rent across the states. Do all the necessary operations to make the chart visualize the tidied data well.\n\nHint: The successful data will have six columns: “GEOID”, “NAME”, “estimate_income”, “estimate_rent”, “moe_income”, and “moe_rent.” Assign “us_rent_income_wider” to the tydy data and print it out. whereas “names_from” argument has one variable – “variable”, “values_from” argument has two variables – “estimate” and “moe.” Multiple variables can be combined with c() function like this: c(estimate, moe).\n\n\n# Reshape the data\nus_rent_income_wider &lt;- us_rent_income %&gt;%\n  pivot_wider(\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n# Print the transformed dataset\nprint(us_rent_income_wider)\n\n# A tibble: 52 × 6\n   GEOID NAME                 estimate_income estimate_rent moe_income moe_rent\n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 01    Alabama                        24476           747        136        3\n 2 02    Alaska                         32940          1200        508       13\n 3 04    Arizona                        27517           972        148        4\n 4 05    Arkansas                       23789           709        165        5\n 5 06    California                     29454          1358        109        3\n 6 08    Colorado                       32401          1125        109        5\n 7 09    Connecticut                    35326          1123        195        5\n 8 10    Delaware                       31560          1076        247       10\n 9 11    District of Columbia           43198          1424        681       17\n10 12    Florida                        25952          1077         70        3\n# ℹ 42 more rows\n\n# Scatter plot of Income vs. Rent\nggplot(us_rent_income_wider, aes(x = estimate_income, y = estimate_rent)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.7) +  # Blue points for visibility\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +  # Linear regression line\n  labs(\n    title = \"Relationship Between Income and Rent Across States\",\n    x = \"Estimated Income ($)\",\n    y = \"Estimated Rent ($)\",\n    caption = \"Data Source: tidyr::us_rent_income\"\n  ) +\n  theme_minimal() +  # Clean and modern theme\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe scatter plot is useful in determining the relationship between the annual rent and the monthly rent. There appears to be a high correlation between the values and it is likely that rental prices will increase as income is higher."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#next-separate-the-number-of-the-stats-column-into-points-assists-and-steals.-assign-this-data-frame-to-a-new-name-mba_sep.-print-out-the-data-set.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#next-separate-the-number-of-the-stats-column-into-points-assists-and-steals.-assign-this-data-frame-to-a-new-name-mba_sep.-print-out-the-data-set.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n5.1 Next, separate the number of the stats column into “points,” “assists,” and “steals.” Assign this data frame to a new name, “mba_sep”. Print out the data set.",
    "text": "5.1 Next, separate the number of the stats column into “points,” “assists,” and “steals.” Assign this data frame to a new name, “mba_sep”. Print out the data set.\n\n# Separate the stats column into points, assists, and steals\nmba_sep &lt;- mba %&gt;%\n  separate(stats, into = c(\"points\", \"assists\", \"steals\"), sep = \"/\")\n\n# Print the transformed dataset\nprint(mba_sep)\n\n  player year points assists steals\n1      A    1     22       2      3\n2      A    2     29       3      4\n3      B    1     18       6      7\n4      B    2     11       1      2\n5      C    1     12       1      1\n6      C    2     19       2      4"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#reverse-the-last-question-by-uniting-three-columns-points-assists-and-steals-into-one-column.-assign-mba_uni-to-the-data-frame.-then-confirm-the-work-by-printing-it-out-on-the-screen.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#reverse-the-last-question-by-uniting-three-columns-points-assists-and-steals-into-one-column.-assign-mba_uni-to-the-data-frame.-then-confirm-the-work-by-printing-it-out-on-the-screen.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n5.2 Reverse the last question by uniting three columns – points, assists and steals – into one column. Assign “mba_uni” to the data frame. Then confirm the work by printing it out on the screen.",
    "text": "5.2 Reverse the last question by uniting three columns – points, assists and steals – into one column. Assign “mba_uni” to the data frame. Then confirm the work by printing it out on the screen.\n\n# Unite the three columns back into one\nmba_uni &lt;- mba_sep %&gt;%\n  unite(stats, points, assists, steals, sep = \"/\")\n\n# Print the transformed dataset\nprint(mba_uni)\n\n  player year  stats\n1      A    1 22/2/3\n2      A    2 29/3/4\n3      B    1 18/6/7\n4      B    2 11/1/2\n5      C    1 12/1/1\n6      C    2 19/2/4"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#filter-the-data-by-including-only-the-flights-that-were-not-canceled.-for-each-day-calculate-the-first-and-last-departure-times-from-nyc-airport.-then-for-each-month-select-one-day-such-that-the-first-flight-departure-was-the-latest-for-the-month.-do-the-necessary-wrangling-and-show-the-results-in-descending-order-of-first-departure-time-in-a-table-using-the-gtgtextras.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#filter-the-data-by-including-only-the-flights-that-were-not-canceled.-for-each-day-calculate-the-first-and-last-departure-times-from-nyc-airport.-then-for-each-month-select-one-day-such-that-the-first-flight-departure-was-the-latest-for-the-month.-do-the-necessary-wrangling-and-show-the-results-in-descending-order-of-first-departure-time-in-a-table-using-the-gtgtextras.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n6.1 Filter the data by including only the flights that were not canceled. For each day, calculate the first and last departure times from NYC airport. Then, for each month, select one day such that the first flight departure was the latest for the month. Do the necessary wrangling and show the results in descending order of “first departure time” in a table using the gt/gtExtras.\n",
    "text": "6.1 Filter the data by including only the flights that were not canceled. For each day, calculate the first and last departure times from NYC airport. Then, for each month, select one day such that the first flight departure was the latest for the month. Do the necessary wrangling and show the results in descending order of “first departure time” in a table using the gt/gtExtras.\n\n\nHints: You can use group_by() and summarize(), filter(), ungroup(), and arrange() functions. In the end, you should have 12 rows of data frame consisting of columns such as year, month, day, first_dep, last_dep, and n.\n\n\n# load the gt/gtextras function \nlibrary(gt)\nlibrary(gtExtras)\n\n# Filter out canceled flights (i.e., keep only flights with non-missing departure times)\nflights_filtered &lt;- flights %&gt;%\n  filter(!is.na(dep_time))\n\n# Calculate first and last departure times for each day\ndaily_departures &lt;- flights_filtered %&gt;%\n  group_by(year, month, day) %&gt;%\n  summarize(\n    first_dep = min(dep_time, na.rm = TRUE),\n    last_dep = max(dep_time, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Select the day in each month with the latest \"first departure\"\nlatest_first_dep_per_month &lt;- daily_departures %&gt;%\n  group_by(year, month) %&gt;%\n  slice_max(first_dep, n = 1) %&gt;% # Select the row with the maximum first_dep in each month\n  ungroup() %&gt;%\n  arrange(desc(first_dep))  # Arrange in descending order\n\n# Display results using gt\nlatest_first_dep_per_month %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Latest First Departure per Month\",\n    subtitle = \"NYC Flights (Non-Canceled Flights Only)\"\n  ) %&gt;%\n  fmt_time(columns = c(first_dep, last_dep), time_style = \"hms\") %&gt;%\n  cols_label(\n    year = \"Year\",\n    month = \"Month\",\n    day = \"Day\",\n    first_dep = \"First Departure\",\n    last_dep = \"Last Departure\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 18,\n    heading.subtitle.font.size = 14\n  )\n\n\n\n\n\n\nLatest First Departure per Month\n\n\nNYC Flights (Non-Canceled Flights Only)\n\n\nYear\nMonth\nDay\nFirst Departure\nLast Departure\n\n\n\n\n2013\n2\n9\n901\n2358\n\n\n2013\n1\n20\n525\n2356\n\n\n2013\n9\n29\n521\n2356\n\n\n2013\n10\n6\n519\n2346\n\n\n2013\n11\n3\n519\n2358\n\n\n2013\n3\n3\n509\n2353\n\n\n2013\n12\n18\n500\n2358\n\n\n2013\n5\n29\n457\n2352\n\n\n2013\n6\n15\n456\n2356\n\n\n2013\n4\n30\n455\n2351\n\n\n2013\n8\n31\n455\n2359\n\n\n2013\n7\n5\n38\n2358\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nYes, it orders the table in order with first departure of each month. This gives us an interesting order of the data that can be helpful in interpretation."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#wrangle-the-data-to-find-out-the-top-5-destinations-to-which-carriers-flew-the-most.-print-the-data-in-a-nice-table.-does-the-finding-make-sense-discuss-the-outcome.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#wrangle-the-data-to-find-out-the-top-5-destinations-to-which-carriers-flew-the-most.-print-the-data-in-a-nice-table.-does-the-finding-make-sense-discuss-the-outcome.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n6.2 Wrangle the data to find out the top 5 destinations to which carriers flew the most. Print the data in a nice table. Does the finding make sense; discuss the outcome.",
    "text": "6.2 Wrangle the data to find out the top 5 destinations to which carriers flew the most. Print the data in a nice table. Does the finding make sense; discuss the outcome.\n\n# Find the top 5 destinations with the most flights\ntop_5_destinations &lt;- flights %&gt;%\n  filter(!is.na(dest)) %&gt;%  # Ensure no missing destination values\n  count(dest, sort = TRUE) %&gt;%  # Count flights per destination\n  top_n(5, n)  # Select the top 5 destinations\n\n# Display results in a formatted table using gt\ntop_5_destinations %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Top 5 Flight Destinations\",\n    subtitle = \"Most Frequent Destinations from NYC (2013)\"\n  ) %&gt;%\n  cols_label(\n    dest = \"Destination Airport\",\n    n = \"Total Flights\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = px(14),\n    heading.title.font.size = px(18),\n    heading.subtitle.font.size = px(14)\n  )\n\n\n\n\n\n\nTop 5 Flight Destinations\n\n\nMost Frequent Destinations from NYC (2013)\n\n\nDestination Airport\nTotal Flights\n\n\n\n\nORD\n17283\n\n\nATL\n17215\n\n\nLAX\n16174\n\n\nBOS\n15508\n\n\nMCO\n14082\n\n\n\n\n\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nThe findings of the most traveled to airports in the US is not extremely unusual. We see that there is common travel to Los Angeles, Chicago, Boston, and Orlando. Out of the major cities, it would only appear unusual that Boston and Orlando are on the list, but it is likely that these airports are either layovers for the cities or central airports for air traffic. Specifically, anyone flying into Boston likely has a layover in NYC, and anyone flying to the south eastern US has a layover in Orlando airport. If these are true, that would suggest that the data is accurate. With LA, Chicago, and Atlanta; these cities are large metros that have a lot of economic movement so there is no surprise there. You could even say that Atlanta is a major airport hub for layovers similar to Dallas and Orlando."
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#using-the-data-frame-air.tib-find-out-how-many-missing-values-exist-in-the-column-solar.r-and-what-percent-of-the-record-the-missing-values-account-for.-in-doing-so-try-tidyverse-way-of-coding.-next-create-a-variable-called-solar_mean-and-assign-the-mean-of-the-column-solar.r-to-it.-finally-replace-all-the-missing-values-in-the-column-solar.r-with-soar_mean.-confirm-that-there-are-no-more-missing-values-in-this-column.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#using-the-data-frame-air.tib-find-out-how-many-missing-values-exist-in-the-column-solar.r-and-what-percent-of-the-record-the-missing-values-account-for.-in-doing-so-try-tidyverse-way-of-coding.-next-create-a-variable-called-solar_mean-and-assign-the-mean-of-the-column-solar.r-to-it.-finally-replace-all-the-missing-values-in-the-column-solar.r-with-soar_mean.-confirm-that-there-are-no-more-missing-values-in-this-column.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n7.1 Using the data frame “air.tib,” find out how many missing values exist in the column “Solar.R and what percent of the record the missing values account for”. In doing so, try Tidyverse way of coding. Next, create a variable called “Solar_Mean” and assign the mean of the column “Solar.R” to it. Finally, replace all the missing values in the column “Solar.R” with “Soar_Mean”. Confirm that there are no more missing values in this column.",
    "text": "7.1 Using the data frame “air.tib,” find out how many missing values exist in the column “Solar.R and what percent of the record the missing values account for”. In doing so, try Tidyverse way of coding. Next, create a variable called “Solar_Mean” and assign the mean of the column “Solar.R” to it. Finally, replace all the missing values in the column “Solar.R” with “Soar_Mean”. Confirm that there are no more missing values in this column.\n\n# Count missing values and calculate percentage\nmissing_count &lt;- air.tib %&gt;%\n  summarise(missing = sum(is.na(Solar.R)),\n            total = n(),\n            percent_missing = (missing / total) * 100)\n\n# Print missing value count and percentage\nprint(missing_count)\n\n# A tibble: 1 × 3\n  missing total percent_missing\n    &lt;int&gt; &lt;int&gt;           &lt;dbl&gt;\n1       7   153            4.58\n\n# Compute mean of Solar.R excluding NAs\nSolar_Mean &lt;- air.tib %&gt;%\n  summarise(Solar_Mean = mean(Solar.R, na.rm = TRUE)) %&gt;%\n  pull(Solar_Mean)\n\n# Replace missing values in Solar.R with the computed mean\nair.tib &lt;- air.tib %&gt;%\n  mutate(Solar.R = ifelse(is.na(Solar.R), Solar_Mean, Solar.R))\n\n# Confirm that there are no more missing values\nmissing_check &lt;- sum(is.na(air.tib$Solar.R))\nprint(paste(\"Missing values in Solar.R after replacement:\", missing_check))\n\n[1] \"Missing values in Solar.R after replacement: 0\""
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#use-the-air.tib-data-frame.-here-the-goal-is-to-visualize-the-monthly-total-temperature-silly-with-a-barplot-with-the-month-on-the-y-axis-and-the-total-temperature-on-the-x-axis.-to-do-so-do-the-appropriate-wrangling-and-visualization-in-one-chain-of-codes.-to-do-so-do-the-following.",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#use-the-air.tib-data-frame.-here-the-goal-is-to-visualize-the-monthly-total-temperature-silly-with-a-barplot-with-the-month-on-the-y-axis-and-the-total-temperature-on-the-x-axis.-to-do-so-do-the-appropriate-wrangling-and-visualization-in-one-chain-of-codes.-to-do-so-do-the-following.",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n8.1 Use the air.tib data frame. Here, the goal is to visualize the monthly total temperature (silly?) with a barplot with the Month on the y-axis and the total temperature on the x-axis. To do so, do the appropriate wrangling and visualization in one chain of codes. To do so, do the following.",
    "text": "8.1 Use the air.tib data frame. Here, the goal is to visualize the monthly total temperature (silly?) with a barplot with the Month on the y-axis and the total temperature on the x-axis. To do so, do the appropriate wrangling and visualization in one chain of codes. To do so, do the following.\n\nTo create the total monthly temperature, count the number of months weighted by Temp.\nChange Month’s data from numeric value to associated character: from 5 to “May”, 6 to “June”, 7 to “July”, 8 to “August”, and 9 to “September”.\nConvert Month to a factor.\nReorder Month by the size of the total temperature in descending order.\n\nAfter you transformed the data s above in two columns – Month and n, you can visualize the table in a barplot. As usual, do the necessary beautification of the chart to make it presentable to the audience. There are at least two ways to wrangle to get the same outcome. If you can do both, you will get a bonus point.\n\n# Wrangle the data and visualize it in one chain\nair.tib %&gt;%\n  # 1. Calculate total temperature by month\n  group_by(Month) %&gt;%\n  summarise(total_temp = sum(Temp, na.rm = TRUE)) %&gt;%\n  # 2. Change Month from numeric to character\n  mutate(Month = recode(Month, `5` = \"May\", `6` = \"June\", `7` = \"July\", `8` = \"August\", `9` = \"September\")) %&gt;%\n  # 3. Convert Month to factor\n  mutate(Month = factor(Month, levels = c(\"May\", \"June\", \"July\", \"August\", \"September\"))) %&gt;%\n  # 4. Reorder Month by total temperature in descending order\n  arrange(desc(total_temp)) %&gt;%\n  # 5. Visualize using a barplot\n  ggplot(aes(x = total_temp, y = reorder(Month, total_temp), fill = Month)) +\n  geom_col(show.legend = FALSE) + # Bar chart\n  labs(\n    title = \"Total Temperature by Month\",\n    x = \"Total Temperature\",\n    y = \"Month\"\n  ) +\n  theme_minimal() + # Clean theme\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    axis.text.x = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.title.x = element_text(size = 14),\n    axis.title.y = element_text(size = 14)\n  )"
  },
  {
    "objectID": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#bonus-point",
    "href": "listings/M05 2 Application Data Wrangling with Tidyverse in R.html#bonus-point",
    "title": "M05-2-Application: Data Wrangling with Tidyverse in R",
    "section": "\n8.2 Bonus Point",
    "text": "8.2 Bonus Point\n\nlibrary(forcats)\n\nair.tib %&gt;%\n  group_by(Month) %&gt;%\n  summarise(total_temp = sum(Temp, na.rm = TRUE)) %&gt;%\n  mutate(Month = recode(Month, `5` = \"May\", `6` = \"June\", `7` = \"July\", `8` = \"August\", `9` = \"September\")) %&gt;%\n  mutate(Month = fct_reorder(Month, total_temp, .desc = TRUE)) %&gt;%\n  ggplot(aes(x = total_temp, y = Month, fill = Month)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Total Temperature by Month\", x = \"Total Temperature\", y = \"Month\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5))"
  },
  {
    "objectID": "listings/M11-2-Application-R_Programming.html",
    "href": "listings/M11-2-Application-R_Programming.html",
    "title": "M11-2-Application-R_Programing.qmd",
    "section": "",
    "text": "Question 1\n\nCreate a vector “quantity” and assign 50 to it. If the quantity is greater than 20, your code should print “You sold a lot”; otherwise, print “Not enough for today.”\n\n\n# Create a vector named \"quantity\"\nquantity &lt;- 50\n\n# Check if quantity is greater than 20\nif (quantity &gt; 20) {\n  print(\"You sold a lot\")\n} else {\n  print(\"Not enough for today.\")\n}\n\n[1] \"You sold a lot\"\n\n\nQuestion 2\n\nCreate a vector “sales” and assign the value 25 to it. If the value is less than 20, your code should print “Not enough for today.” If the quantity sold is between 20 and 30 inclusively, print “Average Day.” If your sales amount is greater than 30, print out “You had a great day!”\n\n\n# Create a vector named \"sales\"\nsales &lt;- 25\n\n# Conditional checks\nif (sales &lt; 20) {\n  print(\"Not enough for today.\")\n} else if (sales &gt;= 20 && sales &lt;= 30) {\n  print(\"Average Day.\")\n} else {\n  print(\"You had a great day!\")\n}\n\n[1] \"Average Day.\"\n\n\nQuestion 3\n\nEach product has its own category and also the respective tax rate applied to it. Therefore, products in category “A” are taxed by 8%, category “B” by 10%, and category “C” by 20%. Create a vector, “category,” and assign a value “A” to the vector. Also, create a vector “price” and assign it 100. Create an if statement that identifies the categories and applies the tax rate to the price of this product. In the end, print the tax rate applied and the final price. For example, for a product in the “B” category (price=10), your printout must be: “A tax rate of 10% is applied. The total price is 11.”\n\n\n# Create vectors\ncategory &lt;- \"A\"\nprice &lt;- 100\n\n# Apply tax rate based on category\nif (category == \"A\") {\n  tax_rate &lt;- 0.08\n} else if (category == \"B\") {\n  tax_rate &lt;- 0.10\n} else if (category == \"C\") {\n  tax_rate &lt;- 0.20\n} else {\n  tax_rate &lt;- 0\n}\n\n# Calculate total price\ntotal_price &lt;- price * (1 + tax_rate)\n\n# Print the results\nprint(paste0(\"A tax rate of \", tax_rate * 100, \"% is applied. The total price is \", total_price, \".\"))\n\n[1] \"A tax rate of 8% is applied. The total price is 108.\"\n\n\nQuestion 4\n\nCreate a for loop to count the even numbers in a vector q4. This vector must contain the values 2, 5, 3, 9, 8, 11, and 6. Your output must be 3.\n\n\n# Create the vector q4\nq4 &lt;- c(2, 5, 3, 9, 8, 11, 6)\n\n# Initialize counter\neven_count &lt;- 0\n\n# For loop to count even numbers\nfor (num in q4) {\n  if (num %% 2 == 0) {\n    even_count &lt;- even_count + 1\n  }\n}\n\n# Print the number of even numbers\nprint(even_count)\n\n[1] 3\n\n\nQuestion 5\n\nCreate a vector q5 in the range of 1 to 8. For each one of the numbers in the vector, if it is greater than 4 and even, print it. Otherwise, print “Condition not satisfied”. Your output must have 8 lines, showing the numbers 6 and 8 in two of the eight lines.\n\n\n# Create the vector q5\nq5 &lt;- 1:8\n\n# For loop to check conditions\nfor (num in q5) {\n  if (num &gt; 4 && num %% 2 == 0) {\n    print(num)\n  } else {\n    print(\"Condition not satisfied\")\n  }\n}\n\n[1] \"Condition not satisfied\"\n[1] \"Condition not satisfied\"\n[1] \"Condition not satisfied\"\n[1] \"Condition not satisfied\"\n[1] \"Condition not satisfied\"\n[1] 6\n[1] \"Condition not satisfied\"\n[1] 8\n\n\nQuestion 6\n\nCreate a function called “pow” that takes two arguments: x and y. The first argument is raised to the power of the second one, and prints the result like this: “8 raised to the power of 2 is 64.” Lastly, call the function with those values after creating the function.\n\n\n# Create the function \"pow\"\npow &lt;- function(x, y) {\n  result &lt;- x^y\n  print(paste0(x, \" raised to the power of \", y, \" is \", result, \".\"))\n}\n\n# Call the function with x = 8 and y = 2\npow(8, 2)\n\n[1] \"8 raised to the power of 2 is 64.\"\n\n\nQuestion 7\n\nQ7.1. Run this line of code: df &lt;- data.frame(x = 1:4, y = 5:8, z = 10:13). Use the apply function to get the mean and sum of each ROW on this dataframe. Add them to the data frame.\n\n\n# Create the dataframe\ndf &lt;- data.frame(x = 1:4, y = 5:8, z = 10:13)\n\n# Calculate row means and sums using apply()\nrow_means &lt;- apply(df, 1, mean)\nrow_sums &lt;- apply(df, 1, sum)\n\n# Add the results to the original dataframe\ndf$mean &lt;- row_means\ndf$sum &lt;- row_sums\n\n# Print the updated dataframe\nprint(df)\n\n  x y  z     mean sum\n1 1 5 10 5.333333  16\n2 2 6 11 6.333333  19\n3 3 7 12 7.333333  22\n4 4 8 13 8.333333  25\n\n\n\nQ7.2. Create a vector called q8 and assign the values 12, 18, and 6. Next, use the apply family and map family functions to calculate the square root of each element.\n\n\n# Create the vector q8\nq8 &lt;- c(12, 18, 6)\n\n# Using sapply (apply family) to calculate the square root\nsqrt_sapply &lt;- sapply(q8, sqrt)\nprint(sqrt_sapply)\n\n[1] 3.464102 4.242641 2.449490\n\n# Using purrr::map (map family) to calculate the square root\nlibrary(purrr)\nsqrt_map &lt;- map_dbl(q8, sqrt)  # map_dbl because we want numeric output\nprint(sqrt_map)\n\n[1] 3.464102 4.242641 2.449490\n\n\n\nQ7.3. Run this code below: q9 &lt;- list(A = 1:5, B = 6:20, C = 1). Use the apply family and map family functions to calculate the number of components of each element of this list. Hint: Use the length function.\n\n\n# Create the list q9\nq9 &lt;- list(A = 1:5, B = 6:20, C = 1)\n\n# Using sapply (apply family) to calculate the length of each element in the list\nlength_sapply &lt;- sapply(q9, length)\nprint(length_sapply)\n\n A  B  C \n 5 15  1 \n\n# Using purrr::map (map family) to calculate the length of each element\nlibrary(purrr)\nlength_map &lt;- map_int(q9, length)  # map_int because we want integer output\nprint(length_map)\n\n A  B  C \n 5 15  1 \n\n\n\nQ7.4. What is the difference between lapply and sapply functions. Which map functions are similar to them? Consider that you want to calculate the exponentials of three numbers. In this case, which function you should use and why? Type your answer as a comment?\n\n\n# Difference between lapply and sapply:\n\n# lapply() returns a list regardless of the output structure, even if the results can be simplified.\n# sapply() attempts to simplify the output (for example, into a vector, matrix, or array) whenever possible.\n\n# Map equivalents:\n# - lapply() is similar to map() (both return a list).\n# - sapply() is similar to map_lgl(), map_int(), map_dbl(), or map_chr(), depending on the desired output type.\n\n# To calculate exponentials of three numbers:\nnumbers &lt;- c(2, 3, 4)\n\n# Using lapply() (returns a list)\nexp_lapply &lt;- lapply(numbers, exp)\nprint(exp_lapply)\n\n[[1]]\n[1] 7.389056\n\n[[2]]\n[1] 20.08554\n\n[[3]]\n[1] 54.59815\n\n# Using sapply() (returns a vector)\nexp_sapply &lt;- sapply(numbers, exp)\nprint(exp_sapply)\n\n[1]  7.389056 20.085537 54.598150\n\n# The best function to use depends on the desired output:\n# If you want a list of the results, use lapply().\n# If you want a simplified output like a numeric vector, use sapply().\n\n# In this case, I would recommend using sapply() because the result is numeric and simplified (vector), \n# which is more appropriate when you're calculating exponentials for a set of numbers and need a vector of results.\n\nQuestion 8\n\nQ8.1: First, execute the following codes in the code chunk to read the data into R and set the global theme.\n\n\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nstudent_ratio &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/student_teacher_ratio.csv\")\n\n\nQ8.2. Explore the data. Especially count the number of “year”, the “indicator”, and “country”, separately.\n\n\n# Count the number of unique years\nyear_count &lt;- student_ratio %&gt;%\n  count(year)\n\n# Count the number of unique indicators\nindicator_count &lt;- student_ratio %&gt;%\n  count(indicator)\n\n# Count the number of unique countries\ncountry_count &lt;- student_ratio %&gt;%\n  count(country)\n\n# Display the counts\nyear_count\n\n\n  \n\n\nindicator_count\n\n\n  \n\n\ncountry_count\n\n\n  \n\n\n\n\nQ8.3. Next, draw a chart that shows the top and bottom 10 countries in student-teacher ratio for 2012.\n\n\n# Inspect column names to confirm the correct column name for the ratio\ncolnames(student_ratio)\n\n[1] \"edulit_ind\"    \"indicator\"     \"country_code\"  \"country\"      \n[5] \"year\"          \"student_ratio\" \"flag_codes\"    \"flags\"        \n\n# Filter the data for the year 2012\ndata_2012 &lt;- student_ratio %&gt;%\n  filter(year == 2012) %&gt;%\n  arrange(desc(student_ratio))  # Sort by the student-teacher ratio\n\n# Get the top and bottom 10 countries\ntop_bottom_10 &lt;- bind_rows(\n  head(data_2012, 10),  # Top 10 countries\n  tail(data_2012, 10)   # Bottom 10 countries\n)\n\n# Create a bar chart to visualize the top and bottom 10 countries\nggplot(top_bottom_10, aes(x = reorder(country, student_ratio), y = student_ratio, fill = student_ratio &gt; 1)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip the coordinates for better readability\n  labs(\n    title = \"Top and Bottom 10 Countries in Student-Teacher Ratio (2012)\",\n    x = \"Country\",\n    y = \"Student-Teacher Ratio\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQ8.4. Note that there are six years from 2012 to 2017 that have a significant number of observations, conducive for drawing charts by year. Since you don’t want to copy and paste the code again and again, you want to create a function that allows you to use it to draw all the charts by simply entering “year” information into the function. You will want to show “year” information in the chart as a subtitle. Create such a function.\n\n\n# Define the function to create a chart for a given year\ncreate_student_teacher_chart &lt;- function(year) {\n  # Filter the data for the specified year\n  data_year &lt;- student_ratio %&gt;%\n    filter(year == year) %&gt;%\n    arrange(desc(student_ratio))  # Sort by the student-teacher ratio\n  \n  # Get the top and bottom 10 countries\n  top_bottom_10 &lt;- bind_rows(\n    head(data_year, 10),  # Top 10 countries\n    tail(data_year, 10)   # Bottom 10 countries\n  )\n  \n  # Create the bar chart\n  ggplot(top_bottom_10, aes(x = reorder(country, student_ratio), y = student_ratio, fill = student_ratio &gt; 1)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +  # Flip the coordinates for better readability\n    labs(\n      title = paste(\"Top and Bottom 10 Countries in Student-Teacher Ratio\"),\n      subtitle = paste(\"Year:\", year),  # Add the year as a subtitle\n      x = \"Country\",\n      y = \"Student-Teacher Ratio\"\n    ) +\n    theme_minimal()\n}\n\n# Example usage: Call the function for the year 2012\ncreate_student_teacher_chart(2012)\n\n\n\n\n\n\n\n\nQ8.5. Draw all charts for 2012 through 2017 using the function.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Remove rows with missing student_ratio values\nstudent_ratio_clean &lt;- na.omit(student_ratio)\n\n# Check the structure of the cleaned data\nsummary(student_ratio_clean)\n\n  edulit_ind         indicator         country_code         country         \n Length:679         Length:679         Length:679         Length:679        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n      year      student_ratio      flag_codes           flags          \n Min.   :2012   Min.   :  2.418   Length:679         Length:679        \n 1st Qu.:2013   1st Qu.: 14.332   Class :character   Class :character  \n Median :2014   Median : 18.785   Mode  :character   Mode  :character  \n Mean   :2015   Mean   : 20.998                                        \n 3rd Qu.:2016   3rd Qu.: 23.800                                        \n Max.   :2017   Max.   :155.000                                        \n\n# Filter the data for 2012\ndata_2012 &lt;- student_ratio_clean %&gt;%\n  filter(year == 2012) %&gt;%\n  arrange(desc(student_ratio))\n\n# Get the top and bottom 10 countries\ntop_bottom_10 &lt;- bind_rows(\n  head(data_2012, 10),  # Top 10 countries\n  tail(data_2012, 10)   # Bottom 10 countries\n)\n\n# Create a bar chart for 2012\nggplot(top_bottom_10, aes(x = reorder(country, student_ratio), y = student_ratio, fill = student_ratio &gt; 1)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Top and Bottom 10 Countries in Student-Teacher Ratio\",\n    subtitle = \"Year: 2012\",\n    x = \"Country\",\n    y = \"Student-Teacher Ratio\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nQuestion 9\n\nQ9.0. Read the following data in R and be prepared for analysis.\n\n\nanimal_outcomes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-21/animal_outcomes.csv')\nanimal_complaints &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-21/animal_complaints.csv')\nbrisbane_complaints &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-21/brisbane_complaints.csv')\n\n\nQ9.1. First, briefly take a look at each data set. You will notice that some variables are lower case while others are upper case. You will also notice that some variables are consist of more than one word.\n\n\n# View the structure of each dataset to check variables and data types\nstr(animal_outcomes)\n\nspc_tbl_ [664 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year       : num [1:664] 1999 1999 1999 1999 1999 ...\n $ animal_type: chr [1:664] \"Dogs\" \"Dogs\" \"Dogs\" \"Dogs\" ...\n $ outcome    : chr [1:664] \"Reclaimed\" \"Rehomed\" \"Other\" \"Euthanized\" ...\n $ ACT        : num [1:664] 610 1245 12 360 111 ...\n $ NSW        : num [1:664] 3140 7525 745 9221 201 ...\n $ NT         : num [1:664] 205 526 955 9 22 269 0 847 1 3 ...\n $ QLD        : num [1:664] 1392 5489 860 9214 206 ...\n $ SA         : num [1:664] 2329 1105 380 1701 157 ...\n $ TAS        : num [1:664] 516 480 168 599 31 ...\n $ VIC        : num [1:664] 7130 4908 1001 5217 884 ...\n $ WA         : num [1:664] 1 137 6 18 0 62 5 5 0 0 ...\n $ Total      : num [1:664] 15323 21415 4127 26339 1612 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   animal_type = col_character(),\n  ..   outcome = col_character(),\n  ..   ACT = col_double(),\n  ..   NSW = col_double(),\n  ..   NT = col_double(),\n  ..   QLD = col_double(),\n  ..   SA = col_double(),\n  ..   TAS = col_double(),\n  ..   VIC = col_double(),\n  ..   WA = col_double(),\n  ..   Total = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(animal_complaints)\n\nspc_tbl_ [42,413 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Animal Type       : chr [1:42413] \"dog\" \"dog\" \"dog\" \"dog\" ...\n $ Complaint Type    : chr [1:42413] \"Aggressive Animal\" \"Noise\" \"Noise\" \"Private Impound\" ...\n $ Date Received     : chr [1:42413] \"June 2020\" \"June 2020\" \"June 2020\" \"June 2020\" ...\n $ Suburb            : chr [1:42413] \"Alice River\" \"Alice River\" \"Alice River\" \"Alice River\" ...\n $ Electoral Division: chr [1:42413] \"Division 1\" \"Division 1\" \"Division 1\" \"Division 1\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Animal Type` = col_character(),\n  ..   `Complaint Type` = col_character(),\n  ..   `Date Received` = col_character(),\n  ..   Suburb = col_character(),\n  ..   `Electoral Division` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(brisbane_complaints)\n\nspc_tbl_ [31,330 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ nature            : chr [1:31330] \"Animal\" \"Animal\" \"Animal\" \"Animal\" ...\n $ animal_type       : chr [1:31330] \"Dog\" \"Dog\" \"Dog\" \"Dog\" ...\n $ category          : chr [1:31330] \"Fencing Issues\" \"Fencing Issues\" \"Defecating In Public\" \"Fencing Issues\" ...\n $ suburb            : chr [1:31330] \"SUNNYBANK\" \"SUNNYBANK HILLS\" \"SUNNYBANK\" \"SUNNYBANK\" ...\n $ date_range        : chr [1:31330] \"1st-quarter-2016-17.csv\" \"1st-quarter-2016-17.csv\" \"1st-quarter-2016-17.csv\" \"1st-quarter-2016-17.csv\" ...\n $ responsible_office: chr [1:31330] NA NA NA NA ...\n $ city              : chr [1:31330] \"Brisbane\" \"Brisbane\" \"Brisbane\" \"Brisbane\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   nature = col_character(),\n  ..   animal_type = col_character(),\n  ..   category = col_character(),\n  ..   suburb = col_character(),\n  ..   date_range = col_character(),\n  ..   responsible_office = col_character(),\n  ..   city = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# View the column names of each dataset to check case and multi-word variable names\ncolnames(animal_outcomes)\n\n [1] \"year\"        \"animal_type\" \"outcome\"     \"ACT\"         \"NSW\"        \n [6] \"NT\"          \"QLD\"         \"SA\"          \"TAS\"         \"VIC\"        \n[11] \"WA\"          \"Total\"      \n\ncolnames(animal_complaints)\n\n[1] \"Animal Type\"        \"Complaint Type\"     \"Date Received\"     \n[4] \"Suburb\"             \"Electoral Division\"\n\ncolnames(brisbane_complaints)\n\n[1] \"nature\"             \"animal_type\"        \"category\"          \n[4] \"suburb\"             \"date_range\"         \"responsible_office\"\n[7] \"city\"              \n\n\n\nQ9.2. Create a pipeable function that will help you clean up the variable names across the data set, clean them up using the function, and reassign the data sets to the same names to overwrite them.\n\n\n# Create a function to clean column names\nclean_colnames &lt;- function(df) {\n  colnames(df) &lt;- tolower(gsub(\" \", \"_\", colnames(df)))\n  return(df)\n}\n\n# Clean up the column names in each dataset using the function\nanimal_outcomes &lt;- animal_outcomes %&gt;% clean_colnames()\nanimal_complaints &lt;- animal_complaints %&gt;% clean_colnames()\nbrisbane_complaints &lt;- brisbane_complaints %&gt;% clean_colnames()\n\n# View the updated column names for verification\ncolnames(animal_outcomes)\n\n [1] \"year\"        \"animal_type\" \"outcome\"     \"act\"         \"nsw\"        \n [6] \"nt\"          \"qld\"         \"sa\"          \"tas\"         \"vic\"        \n[11] \"wa\"          \"total\"      \n\ncolnames(animal_complaints)\n\n[1] \"animal_type\"        \"complaint_type\"     \"date_received\"     \n[4] \"suburb\"             \"electoral_division\"\n\ncolnames(brisbane_complaints)\n\n[1] \"nature\"             \"animal_type\"        \"category\"          \n[4] \"suburb\"             \"date_range\"         \"responsible_office\"\n[7] \"city\"              \n\n\n\nQ9.3. Create a pipeable function that can draw a bar chart showing a count of a variable from any of the three data sets above.\n\n\n# Create a pipeable function to draw a bar chart for a variable\ndraw_bar_chart &lt;- function(df, variable) {\n  df %&gt;%\n    count(!!sym(variable)) %&gt;%  # Count occurrences of each unique value in the variable\n    ggplot(aes(x = !!sym(variable), y = n)) +  # Plot the variable on x and count on y\n    geom_bar(stat = \"identity\") +  # Use identity to plot the count\n    theme_minimal() +  # Clean theme\n    labs(x = variable, y = \"Count\", title = paste(\"Count of\", variable))  # Labels and title\n}\n\n# Example usage:\n# Drawing a bar chart for 'animal_type' from the animal_outcomes dataset\ndraw_bar_chart(animal_outcomes, \"animal_type\")\n\n\n\n\n\n\n# Drawing a bar chart for 'complaint_type' from the animal_complaints dataset\ndraw_bar_chart(animal_complaints, \"complaint_type\")\n\n\n\n\n\n\n# Drawing a bar chart for 'category' from the brisbane_complaints dataset\ndraw_bar_chart(brisbane_complaints, \"category\")\n\n\n\n\n\n\n\n\nQ9.4. Then, using the function, create two bar charts of your interest from each data set.\n\n\n# Load the janitor package\nlibrary(janitor)\n\n# Clean column names in all datasets\nanimal_outcomes &lt;- animal_outcomes %&gt;% clean_names()\nanimal_complaints &lt;- animal_complaints %&gt;% clean_names()\nbrisbane_complaints &lt;- brisbane_complaints %&gt;% clean_names()\n\n# Verify the column names to ensure they are cleaned properly\ncolnames(animal_outcomes)\n\n [1] \"year\"        \"animal_type\" \"outcome\"     \"act\"         \"nsw\"        \n [6] \"nt\"          \"qld\"         \"sa\"          \"tas\"         \"vic\"        \n[11] \"wa\"          \"total\"      \n\ncolnames(animal_complaints)\n\n[1] \"animal_type\"        \"complaint_type\"     \"date_received\"     \n[4] \"suburb\"             \"electoral_division\"\n\ncolnames(brisbane_complaints)\n\n[1] \"nature\"             \"animal_type\"        \"category\"          \n[4] \"suburb\"             \"date_range\"         \"responsible_office\"\n[7] \"city\"              \n\n# Drawing two bar charts for animal_outcomes dataset:\n# 1. Count of animal types\ndraw_bar_chart(animal_outcomes, \"animal_type\")\n\n\n\n\n\n\n# 2. Count of outcomes\ndraw_bar_chart(animal_outcomes, \"outcome\")\n\n\n\n\n\n\n# Drawing two bar charts for animal_complaints dataset:\n# 1. Count of complaint types\ndraw_bar_chart(animal_complaints, \"complaint_type\")\n\n\n\n\n\n\n# 2. Count of animal types\ndraw_bar_chart(animal_complaints, \"animal_type\")\n\n\n\n\n\n\n# Drawing two bar charts for brisbane_complaints dataset:\n# 1. Count of complaint categories\ndraw_bar_chart(brisbane_complaints, \"category\")\n\n\n\n\n\n\n# 2. Count of suburbs\ndraw_bar_chart(brisbane_complaints, \"suburb\")\n\n\n\n\n\n\n\nQuestion 10\n\nIn this question, you will work with “airquality” data set, which is built-in data available from R Studio. The goal is to create a function that calculates average Ozone amount for each month. Hints: Use nested data. You will have to create multiple functions with one function embedded in another. Also, use one of the map() functions.\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(purrr)\n\n# Define the function to calculate average Ozone per month\ncalculate_avg_ozone_per_month &lt;- function(data) {\n  # Helper function to calculate average for a specific month\n  calc_avg_ozone &lt;- function(month) {\n    data %&gt;%\n      filter(Month == month) %&gt;%\n      summarise(avg_ozone = mean(Ozone, na.rm = TRUE)) %&gt;%\n      pull(avg_ozone)\n  }\n  \n  # Get unique months (the months are represented by 5 to 9 in the airquality dataset)\n  months &lt;- unique(data$Month)\n  \n  # Use map to apply the helper function to each month\n  avg_ozone_by_month &lt;- map_dbl(months, calc_avg_ozone)\n  \n  # Return results in a tibble\n  tibble(Month = months, Avg_Ozone = avg_ozone_by_month)\n}\n\n# Apply the function on the airquality data\naverage_ozone &lt;- calculate_avg_ozone_per_month(airquality)\nprint(average_ozone)\n\n# A tibble: 5 × 2\n  Month Avg_Ozone\n  &lt;int&gt;     &lt;dbl&gt;\n1     5      23.6\n2     6      29.4\n3     7      59.1\n4     8      60.0\n5     9      31.4"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html",
    "href": "listings/Jackson, William Group 3.html",
    "title": "Data Wrangling of Final Project",
    "section": "",
    "text": "library(readxl)"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#packages",
    "href": "listings/Jackson, William Group 3.html#packages",
    "title": "Data Wrangling of Final Project",
    "section": "",
    "text": "library(readxl)"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#upload-the-csv-file",
    "href": "listings/Jackson, William Group 3.html#upload-the-csv-file",
    "title": "Data Wrangling of Final Project",
    "section": "Upload the CSV File",
    "text": "Upload the CSV File\n\nThis file will be uploaded from the original CSV files, one will be the data set that will be used for a geographical representation at a national level and then a local level. I will look at specifically areas within the Los Angeles/Orange County for more relevant information.\n\n\n# Define the raw file URL from GitHub\nurl &lt;- \"https://raw.githubusercontent.com/wlj831/Raw-Data-GBA-5910-/refs/heads/main/Cleaned%20Raw%20Data%20File.csv\"\n\n# Load the data directly into R\ndata &lt;- read.csv(url)\n\n# View the first few rows of the data\nhead(data)\n\n\n  \n\n\n# Load data from GitHub using base R\ndata_traffic &lt;- read.csv(\"https://raw.githubusercontent.com/wlj831/Raw-Data-GBA-5910-/refs/heads/main/Cleaned%20Road%20Traffic%20Accidents.csv\")\n\n# View first few rows\nhead(data_traffic)"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#data-cleaning-and-geographical-map",
    "href": "listings/Jackson, William Group 3.html#data-cleaning-and-geographical-map",
    "title": "Data Wrangling of Final Project",
    "section": "Data Cleaning and Geographical Map",
    "text": "Data Cleaning and Geographical Map\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)      # For state map data\nlibrary(viridis)   # Optional: for color scales, or use red-blue as specified\n\n# Clean the data\nClean_Data_Geography &lt;- na.omit(data)\n\n# Keep only State and County\nState_County_Only &lt;- Clean_Data_Geography[, c(\"State\", \"County\")]\n\n# Count accidents by state\naccidents_by_state &lt;- State_County_Only %&gt;%\n  group_by(State) %&gt;%\n  summarize(accident_count = n())\n\n# Make state names lowercase to match map data\naccidents_by_state$region &lt;- tolower(accidents_by_state$State)\n\n# Get map data for US states\nstates_map &lt;- map_data(\"state\")\n\n# Filter out Alaska and Hawaii\nstates_map &lt;- states_map %&gt;% \n  filter(!(region %in% c(\"alaska\", \"hawaii\")))\n\n# Convert state abbreviations (e.g., \"CA\") to full state names\naccidents_by_state$region &lt;- tolower(state.name[match(accidents_by_state$State, state.abb)])\n\n\n# Join accident data with map data\nmap_data_joined &lt;- left_join(states_map, accidents_by_state, by = \"region\")\n\n# --- Plot with custom background ---\nggplot(map_data_joined, aes(long, lat, group = group, fill = accident_count)) +\n  geom_polygon(color = \"white\", size = 0.2) +\n  scale_fill_gradient(low = \"blue\", high = \"red\", na.value = \"gray90\") +\n  labs(title = \"Car Accidents by State (Lower 48)\", fill = \"Accidents\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.background = element_rect(fill = \"#e6f2ff\", color = NA),      # Light blue\n    panel.background = element_rect(fill = \"#f7f7f7\", color = NA),     # Soft gray map area\n    legend.background = element_rect(fill = \"#e6f2ff\", color = NA),    # Match plot bg\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank()\n  )"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#cities-in-orange-and-la-county-wih-most-car-accidents",
    "href": "listings/Jackson, William Group 3.html#cities-in-orange-and-la-county-wih-most-car-accidents",
    "title": "Data Wrangling of Final Project",
    "section": "Cities in Orange and LA County wih Most Car Accidents",
    "text": "Cities in Orange and LA County wih Most Car Accidents\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Step 1: Filter for LA and Orange Counties\nla_oc_data &lt;- Clean_Data_Geography %&gt;%\n  filter(State == \"CA\", County %in% c(\"Los Angeles\", \"Orange\"))\n\n# Step 2: Count accidents by City\naccidents_by_city &lt;- la_oc_data %&gt;%\n  group_by(City) %&gt;%\n  summarize(accident_count = n()) %&gt;%\n  arrange(desc(accident_count))\n\n# Step 3: Select the top 20 cities with the most accidents\ntop_20_cities &lt;- accidents_by_city %&gt;%\n  top_n(20, wt = accident_count)\n\n# Step 4: Plot the bar chart\nggplot(top_20_cities, aes(x = reorder(City, -accident_count), y = accident_count, fill = accident_count)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Top 20 Cities by Accidents in LA & Orange Counties\", x = \"City\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#top-10-cities-with-car-accidents",
    "href": "listings/Jackson, William Group 3.html#top-10-cities-with-car-accidents",
    "title": "Data Wrangling of Final Project",
    "section": "Top 10 Cities with Car Accidents",
    "text": "Top 10 Cities with Car Accidents\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Step 1: Filter for the lower 48 states (excluding Alaska and Hawaii)\nlower_48_data &lt;- Clean_Data_Geography %&gt;%\n  filter(State != \"AK\", State != \"HI\")\n\n# Step 2: Count accidents by City\naccidents_by_city &lt;- lower_48_data %&gt;%\n  group_by(City) %&gt;%\n  summarize(accident_count = n()) %&gt;%\n  arrange(desc(accident_count))\n\n# Step 3: Select the top 10 cities with the most accidents\ntop_10_cities &lt;- accidents_by_city %&gt;%\n  top_n(10, wt = accident_count)\n\n# Step 4: Plot the bar chart\nggplot(top_10_cities, aes(x = reorder(City, -accident_count), y = accident_count, fill = accident_count)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Top 10 Cities by Accidents in the Lower 48 States\", x = \"City\", y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#analysis-of-location-of-car-accidents",
    "href": "listings/Jackson, William Group 3.html#analysis-of-location-of-car-accidents",
    "title": "Data Wrangling of Final Project",
    "section": "Analysis of location of Car Accidents",
    "text": "Analysis of location of Car Accidents\n\nNow we will analyze if there is any causation on the fact that certain types of intersections places on the road have higher frequency of accidents. We will also analysis the severity of these accidents\n\nLoading Data\n\n# View all column names in the dataset\ncolnames(data_traffic)\n\n [1] \"Age_band_of_driver\"      \"Sex_of_driver\"          \n [3] \"Educational_level\"       \"Vehicle_driver_relation\"\n [5] \"Driving_experience\"      \"Lanes_or_Medians\"       \n [7] \"Types_of_Junction\"       \"Road_surface_type\"      \n [9] \"Light_conditions\"        \"Weather_conditions\"     \n[11] \"Type_of_collision\"       \"Vehicle_movement\"       \n[13] \"Pedestrian_movement\"     \"Cause_of_accident\"      \n[15] \"Accident_severity\"      \n\n# View all unique values in 'Types_of_Junction'\nunique(data_traffic$Types_of_Junction)\n\n[1] \"No junction\" \"Y Shape\"     \"Crossing\"    \"O Shape\"     \"Other\"      \n[6] \"Unknown\"     \"T Shape\"     \"X Shape\"    \n\n# Load the necessary libraries\nlibrary(dplyr)\n\n# Count the frequency of each 'Types_of_Junction'\njunction_counts &lt;- data_traffic %&gt;%\n  group_by(Types_of_Junction) %&gt;%\n  summarise(Count = n())\n\n# View the counted data_traffic\nprint(junction_counts)\n\n# A tibble: 8 × 2\n  Types_of_Junction Count\n  &lt;chr&gt;             &lt;int&gt;\n1 Crossing           2177\n2 No junction        3837\n3 O Shape             164\n4 Other               445\n5 T Shape              60\n6 Unknown            1078\n7 X Shape              12\n8 Y Shape            4543\n\n# Load ggplot2 for plotting\nlibrary(ggplot2)\n\n# Create a bar plot of junction counts\nggplot(junction_counts, aes(x = Types_of_Junction, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  labs(title = \"Accidents by Type of Junction\", \n       x = \"Type of Junction\", \n       y = \"Number of Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n# Load the necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Group the data_traffic by Types_of_Junction and Accident_severity\nseverity_by_junction &lt;- data_traffic %&gt;%\n  group_by(Types_of_Junction, Accident_severity) %&gt;%\n  summarise(Accident_Count = n(), .groups = \"drop\")  # Remove grouping after summarization\n\n# Create a bar graph to visualize the accident severity by junction type\nggplot(severity_by_junction, aes(x = Types_of_Junction, y = Accident_Count, fill = factor(Accident_severity))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Accident Severity by Type of Junction\",\n       x = \"Type of Junction\", \n       y = \"Number of Accidents\",\n       fill = \"Accident Severity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#chi-square-test-of-indepedence",
    "href": "listings/Jackson, William Group 3.html#chi-square-test-of-indepedence",
    "title": "Data Wrangling of Final Project",
    "section": "Chi-Square Test of Indepedence",
    "text": "Chi-Square Test of Indepedence\n\nThis will help test wehater accident severity is statiscally independent of the junction type - i.e. whether certain junctions tend to have more severe accidents.\n\n\n# Create a contingency table\ncontingency_table &lt;- table(data_traffic$Types_of_Junction, data_traffic$Accident_severity)\n\n# Perform Chi-Square Test\nchisq_test &lt;- chisq.test(contingency_table)\n\n# View the result\nchisq_test\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 57.181, df = 14, p-value = 3.639e-07"
  },
  {
    "objectID": "listings/Jackson, William Group 3.html#proportional-heatmap",
    "href": "listings/Jackson, William Group 3.html#proportional-heatmap",
    "title": "Data Wrangling of Final Project",
    "section": "Proportional Heatmap",
    "text": "Proportional Heatmap\n\nVisualization of the proportions of accident severity per junction type which types are skewed toward higher severity.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Summarize the data\nbubble_data &lt;- data_traffic %&gt;%\n  group_by(Types_of_Junction, Accident_severity) %&gt;%\n  summarise(Count = n(), .groups = \"drop\")\n\n# Custom red-blue gradient (from blue -&gt; white -&gt; red)\ncustom_colors &lt;- c(\"#08306B\", \"#2171B5\", \"#6BAED6\", \"#F7F7F7\", \"#FC9272\", \"#CB181D\", \"#67000D\")\n\n# Plot with enhanced color and size\nggplot(bubble_data, aes(x = Types_of_Junction, y = factor(Accident_severity), size = Count, fill = Count)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.8) +\n  scale_size(range = c(4, 16)) +\n  scale_fill_gradientn(colors = custom_colors) +\n  labs(\n    title = \"Bubble Heatmap: Accident Severity by Junction Type\",\n    x = \"Type of Junction\",\n    y = \"Accident Severity\",\n    size = \"Number of Accidents\",\n    fill = \"Number of Accidents\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  )"
  },
  {
    "objectID": "listings/M07-2-Application Assignment for R Data Types.html",
    "href": "listings/M07-2-Application Assignment for R Data Types.html",
    "title": "M07-2-Application Assignment for R Data Types",
    "section": "",
    "text": "1 Q1. Vectors\n\n\nCreate a numeric vector called “stock_prices” with the following data points: 42.5, 43.0, 44.1, and 2.7. Next, verify the class of it. (2) Create a character vector called “stock_company” consisting of the following company names: Microsoft, Apple, Facebook, and Tesla. (3) Create a character vector called “stock_ticker” consisting of stock tickers corresponding to the companies in the “stock_company” vector: MSFT, AAPL, FB, and TSLA. Then verify its data type. (4) Create a logical vector called “make_cars” consisting of the four logical values: FALSE, FALSE, FALSE, and TRUE. (5) After that, add 5 to the vector “stock_prices” and assign the result to the same “stock_prices” vector, essentially updating the existing stock prices vector. (6) Name each element in “stock_prices” vector, using the “stock_company” vector that you created above, and print the stock_prirces vector to confirm that each element has a company name associated with it.\n\n\n\n```{r}\n# Creating Character Vectors for \"Stock Prices\" \nstock_price &lt;- c(42.5, 43.0, 44.1, 2.7)\n\n#Verify the class of \"stock_price\" \nclass(stock_price) #should return numeric \n\n# 2 Create a character vector \"stock_company\" \nstock_company &lt;- c(\"Microsoft\", \"Apple\", \"Facebook\", \"Tesla\")\n\n# 3 Create a character vector #stock_ticker\"\nstock_ticker &lt;- c(\"MSFT\", \"AAPL\", \"FB\", \"TSLA\")\n\n# 4 Create a Logical vector \"make_cars\"\nmake_cars &lt;- c(FALSE, FALSE, FALSE, TRUE)\n\n# 5 Add 5 to each element in the \"stock_prices\" and update its reference\nstock_price &lt;- stock_price + 5 \n\n# 6 Name Elements in \"stock_prices\" using \"stock_companies\" vector \nnames(stock_price) &lt;- stock_company\n\n# Print the updated table for confirmation of vector was properly assigned\nprint(stock_price)\n```\n\n[1] \"numeric\"\nMicrosoft     Apple  Facebook     Tesla \n     47.5      48.0      49.1       7.7 \n\n\n\n2 Q2. Creating and cleaning a data frame in base R (0.5 pts)\n\nQ2.1: Create a data frame called “stock_info.df” which is composed of the four vectors created in exercises #2 and #3 above. Verify the number of rows and columns, and also display the summary.\n\n\n```{r}\n# Creating the data frame \"stock_info.df\" \nstock_info.df &lt;- data.frame(\n  stock_company = stock_company,\n  stock_ticker = stock_ticker, \n  stock_price = stock_price,\n  make_cars = make_cars\n)\n\n#Verify the number of rows and columns \ndim(stock_info.df)\n\n#Display the summary of the data frame \nsummary(stock_info.df)\n\n#Print the data frame to check its contents \nprint(stock_info.df)\n```\n\n[1] 4 4\n stock_company      stock_ticker        stock_price    make_cars      \n Length:4           Length:4           Min.   : 7.70   Mode :logical  \n Class :character   Class :character   1st Qu.:37.55   FALSE:3        \n Mode  :character   Mode  :character   Median :47.75   TRUE :1        \n                                       Mean   :38.08                  \n                                       3rd Qu.:48.27                  \n                                       Max.   :49.10                  \n          stock_company stock_ticker stock_price make_cars\nMicrosoft     Microsoft         MSFT        47.5     FALSE\nApple             Apple         AAPL        48.0     FALSE\nFacebook       Facebook           FB        49.1     FALSE\nTesla             Tesla         TSLA         7.7      TRUE\n\n\n\nQ2.2: (1) Let’s suppose that there was a data entry error in “stock_prices” vector such that Tesla’s original stock price was 42.7, not 2.7. Also, note that previously you updated the stock price by adding 5 to all elements in the stock_prices vector, making the updated stock price of Tesla 7.7. (2) Now that we know there was a data entry error, we need to update Tesla’s stock price correctly in the data frame created in #5. In short, we need to replace 7.7 with 47.7, which would have been the correct stock price for Tesla in Exercise #3.4, had there not been an error in the first place. (3) Do the necessary coding to execute the correction to the stock_info.df data frame. Confirm the operation was successful by printing the data frame.\n\n\n```{r}\n# (1) Identify the incorrect Tesla stock price and correct it\n# Tesla's corrected stock price should be 42.7 + 5 = 47.7\n\n# Correct Tesla's stock price in the existing \"stock_price\" column\nstock_info.df$stock_price[stock_info.df$stock_company == \"Tesla\"] &lt;- 47.7\n\n# Print the updated data frame to confirm the change\nprint(stock_info.df)\n```\n\n          stock_company stock_ticker stock_price make_cars\nMicrosoft     Microsoft         MSFT        47.5     FALSE\nApple             Apple         AAPL        48.0     FALSE\nFacebook       Facebook           FB        49.1     FALSE\nTesla             Tesla         TSLA        47.7      TRUE\n\n\n\n3 Q3. Adding a row or column to a data frame in base R\n\nQ3.1: (1) Add a new row to the bottom of the existing “stock_info.df” data frame with the following information: stock_prices = 50.4, stock_company = General Motors, stock_ticker = GM, and make_cars = TRUE. (2) Print out the updated data frame. (3) Do you see the row name of the last row is “1”? You may want to replace this with the correct name, “General Motors.” Print the data frame again to confirm you fixed the problem.\n\nHint: You may create a data frame with its four elements set as described above and then use rbind() to append it to the existing data frame, “stock_info.df”. Also, see how to change row names in RLinks to an external site. to change row names.\n\n```{r}\n# (1) Create a new data frame with the new row data\nnew_row &lt;- data.frame(\n  stock_company = \"General Motors\",\n  stock_ticker = \"GM\",\n  stock_price = 50.4,\n  make_cars = TRUE\n)\n\n# Append the new row to the existing data frame using rbind()\nstock_info.df &lt;- rbind(stock_info.df, new_row)\n\n# (2) Print the updated data frame to confirm the addition\nprint(stock_info.df)\n\n# (3) Rename the last row to \"General Motors\"\nrownames(stock_info.df)[nrow(stock_info.df)] &lt;- \"General Motors\"\n\n# Print the data frame again to confirm the change\nprint(stock_info.df)\n```\n\n           stock_company stock_ticker stock_price make_cars\nMicrosoft      Microsoft         MSFT        47.5     FALSE\nApple              Apple         AAPL        48.0     FALSE\nFacebook        Facebook           FB        49.1     FALSE\nTesla              Tesla         TSLA        47.7      TRUE\n1         General Motors           GM        50.4      TRUE\n                stock_company stock_ticker stock_price make_cars\nMicrosoft           Microsoft         MSFT        47.5     FALSE\nApple                   Apple         AAPL        48.0     FALSE\nFacebook             Facebook           FB        49.1     FALSE\nTesla                   Tesla         TSLA        47.7      TRUE\nGeneral Motors General Motors           GM        50.4      TRUE\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nCreate a new row as a data frame (new_row) containing the required values. Use rbind() to append this new row to stock_info.df. Print the data frame to verify the row was added. Rename the last row using rownames() so that it correctly displays “General Motors”. Print the final data frame to confirm everything looks correct.\n\n\n\n\nQ3.2 (1) Add a new column with the following column name and values: open_price = 47.3, 47.8, 49.5, 47.9 and 55.3. (2) Print out the updated data frame, “stock_info.df” to confirm your work.\n\n\n```{r}\n# (1) Add a new column \"open_price\" with the given values\nstock_info.df$open_price &lt;- c(47.3, 47.8, 49.5, 47.9, 55.3)\n\n# (2) Print the updated data frame to confirm the changes\nprint(stock_info.df)\n```\n\n                stock_company stock_ticker stock_price make_cars open_price\nMicrosoft           Microsoft         MSFT        47.5     FALSE       47.3\nApple                   Apple         AAPL        48.0     FALSE       47.8\nFacebook             Facebook           FB        49.1     FALSE       49.5\nTesla                   Tesla         TSLA        47.7      TRUE       47.9\nGeneral Motors General Motors           GM        50.4      TRUE       55.3\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nAdd a new column open_price to stock_info.df with the specified values. print(stock_info.df) displays the updated data frame to verify the addition.\n\n\n\n\n4 Q4. Creating a new column from existing columns and subsetting\n\nQ4.1: Now, create a new vector called “change_stock_prices_percent” and add it to the existing data frame to show if the stock increased or decreased its price at the end of the day compared to the beginning of the day.\n\nHints: Consider the stock_prices as the closing prices of the stocks for the day. The change_stock_prices_percent can then be calculated as: [(closing price - open price) / open price * 100]. If the outcome is negative value, the stock price has decreased. If the outcome is positive value, stock price has increased. If you are not familiar with the meaning of the operators in the formula above, note the following:\n\n: subtraction\n/ : division\n: multiplication.\n\n\n```{r}\n# (1) Calculate the percentage change in stock prices using the formula\nchange_stock_prices_percent &lt;- ((stock_info.df$stock_price - stock_info.df$open_price) / stock_info.df$open_price) * 100\n\n# (2) Add the new column to the existing data frame\nstock_info.df$change_stock_prices_percent &lt;- change_stock_prices_percent\n\n# (3) Print the updated data frame to confirm the addition\nprint(stock_info.df)\n```\n\n                stock_company stock_ticker stock_price make_cars open_price\nMicrosoft           Microsoft         MSFT        47.5     FALSE       47.3\nApple                   Apple         AAPL        48.0     FALSE       47.8\nFacebook             Facebook           FB        49.1     FALSE       49.5\nTesla                   Tesla         TSLA        47.7      TRUE       47.9\nGeneral Motors General Motors           GM        50.4      TRUE       55.3\n               change_stock_prices_percent\nMicrosoft                        0.4228330\nApple                            0.4184100\nFacebook                        -0.8080808\nTesla                           -0.4175365\nGeneral Motors                  -8.8607595\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nFormula:\nstock_info.df\\(stock_price represents the closing price of the stock.\nstock_info.df\\)open_price represents the opening price. The formula ((closing price - open price) / open price) * 100 calculates the percentage change in stock price. The result of this calculation is stored in a new vector change_stock_prices_percent.\nThis vector is then added as a new column in the existing stock_info.df data frame.\nThe final step is printing the data frame to confirm that the new column has been added successfully.\n\n\n\n\n4.2: Which stock has increased its value and which ones have decreased? Use the function “subset” to select the two types of stocks.\n\n\n```{r}\n# (1) Subset stocks that have increased in value (positive change)\nstocks_increased &lt;- subset(stock_info.df, change_stock_prices_percent &gt; 0)\n\n# (2) Subset stocks that have decreased in value (negative change)\nstocks_decreased &lt;- subset(stock_info.df, change_stock_prices_percent &lt; 0)\n\n# (3) Print the results\nprint(\"Stocks that increased in value:\")\nprint(stocks_increased)\n\nprint(\"Stocks that decreased in value:\")\nprint(stocks_decreased)\n```\n\n[1] \"Stocks that increased in value:\"\n          stock_company stock_ticker stock_price make_cars open_price\nMicrosoft     Microsoft         MSFT        47.5     FALSE       47.3\nApple             Apple         AAPL        48.0     FALSE       47.8\n          change_stock_prices_percent\nMicrosoft                    0.422833\nApple                        0.418410\n[1] \"Stocks that decreased in value:\"\n                stock_company stock_ticker stock_price make_cars open_price\nFacebook             Facebook           FB        49.1     FALSE       49.5\nTesla                   Tesla         TSLA        47.7      TRUE       47.9\nGeneral Motors General Motors           GM        50.4      TRUE       55.3\n               change_stock_prices_percent\nFacebook                        -0.8080808\nTesla                           -0.4175365\nGeneral Motors                  -8.8607595\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nFormula:\nsubset(stock_info.df, change_stock_prices_percent &gt; 0):\nThis selects rows where change_stock_prices_percent is greater than 0, indicating the stock price has increased. subset(stock_info.df, change_stock_prices_percent &lt; 0):\nThis selects rows where change_stock_prices_percent is less than 0, indicating the stock price has decreased. print() is used to display the results for stocks that have increased or decreased in value.\n\n\n\n\n5 Q5. Matrix (0.5 pts)\n\nCreate a matrix named “odd.mat” that consists of odd numbers starting with “1” with 10 columns and 5 rows.\n\n\nHints: You can type 50 odd numbers directly, but this is not the best way. preferably, you can use a function called seq() as follows: seq(from = 1, by = 2, length.out = 50).\n\n\n```{r}\n# Generate the first 50 odd numbers using seq()\nodd_numbers &lt;- seq(from = 1, by = 2, length.out = 50)\n\n# Convert the vector of odd numbers into a matrix with 10 columns and 5 rows\nodd.mat &lt;- matrix(odd_numbers, ncol = 10, nrow = 5)\n\n# Print the matrix to confirm\nprint(odd.mat)\n```\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1   11   21   31   41   51   61   71   81    91\n[2,]    3   13   23   33   43   53   63   73   83    93\n[3,]    5   15   25   35   45   55   65   75   85    95\n[4,]    7   17   27   37   47   57   67   77   87    97\n[5,]    9   19   29   39   49   59   69   79   89    99\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nseq(from = 1, by = 2, length.out = 50) generates a sequence of 50 odd numbers starting from 1 (incrementing by 2). matrix() takes the vector odd_numbers and reshapes it into a matrix with 10 columns and 5 rows. Finally, print(odd.mat) displays the matrix.\n\n\n\n\n6 Q6. Strings: Sentences dataset\n\nThe goal of this question is to generate insights about the frequencies of the use of the definite article, “The” in the English composition.\n\n\n\nFrom “sentences” data that comes with stringr, which is included in Tidyverse package, confirm that you can correctly identify the first word from each sentence correctly by showing the first word from each sentence for all the sentences in the dataset. (2) Then, extract those first words and print out the first 20 words in the list of words extracted. Make sure you delete any white spaces, if any, before and after each extracted word. (3) Out of the words you extracted, how many times does the word, “The,” appear in the list? (4) What proportion of the extracted words does “The” account for? (5) Does the outcome surprise you? Explain why or why not. (6) Calculate the number of sentences that start with the word, “The” again, this time, using only one chain of codes with a series of the piping operator, starting with the “sentences” data set. Next, using a similar operation, calculate the proportion again.\n\n\n\n```{r}\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the sentences dataset from stringr package\ndata(\"sentences\")\n\n# Extract the first word of each sentence and remove white spaces\nfirst_words &lt;- sentences %&gt;%\n  str_extract(\"^\\\\S+\") %&gt;%  # Extract the first non-space sequence (first word)\n  str_trim()  # Remove leading/trailing spaces\n\n# Print the first 20 words from the list of first words\nprint(first_words[1:20])\n\n# Count how many times \"The\" appears in the first words\nthe_count &lt;- sum(tolower(first_words) == \"the\")\n\n# Print the result\nprint(the_count)\n\n# Calculate the proportion of \"The\" in the first words\nthe_proportion &lt;- the_count / length(first_words)\n\n# Print the result\nprint(the_proportion)\n\n# Use piping to calculate the number of sentences starting with \"The\"\nthe_start_count &lt;- sentences %&gt;%\n  str_extract(\"^\\\\S+\") %&gt;%\n  str_trim() %&gt;%\n  tolower() %&gt;%\n  .[ . == \"the\" ] %&gt;%\n  length()\n\n# Print the result\nprint(the_start_count)\n\n# Calculate the proportion using piping\nthe_start_proportion &lt;- the_start_count / length(sentences)\n\n# Print the result\nprint(the_start_proportion)\n```\n\n [1] \"The\"   \"Glue\"  \"It's\"  \"These\" \"Rice\"  \"The\"   \"The\"   \"The\"   \"Four\" \n[10] \"A\"     \"The\"   \"A\"     \"The\"   \"Kick\"  \"Help\"  \"A\"     \"Smoky\" \"The\"  \n[19] \"The\"   \"The\"  \n[1] 256\n[1] 0.3555556\n[1] 256\n[1] 0.3555556\n\n\n\n7 Q7. Strings: Baseball player stats in Lahman dataset\n\nFrom the “People” dataset in the Lahman package, get all John’s and Joe’s from the first name column and display how many Joe’s and John’s there are in the dataset. Visualize the count of the two first names in a chart too. As usual, make the chart presentable.\n\n\nHints: You need to install Lahman package first and access “People” dataset that comes with the package. The dataset has information and statistics about baseball players such as Player names, DOB, and biographical info. This file is to be used to get details about players listed in the Batting, Pitching, and other files where players are identified only by playerID. For more information look up the help.\n\n\n```{r}\n# Load necessary libraries\nlibrary(Lahman)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Extract John's and Joe's from the People dataset\njohn_joe_count &lt;- People %&gt;%\n  filter(nameFirst %in% c(\"John\", \"Joe\")) %&gt;%\n  count(nameFirst)\n\n# Print the count of Johns and Joes\nprint(john_joe_count)\n\n# Create a bar chart of the counts\nggplot(john_joe_count, aes(x = nameFirst, y = n, fill = nameFirst)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(title = \"Count of Baseball Players Named 'John' and 'Joe'\",\n       x = \"First Name\",\n       y = \"Count\",\n       fill = \"First Name\") +\n  theme_minimal() +\n  theme(text = element_text(size = 14)) +\n  scale_fill_manual(values = c(\"John\" = \"steelblue\", \"Joe\" = \"firebrick\"))\n```\n\n  nameFirst   n\n1       Joe 413\n2      John 516\n\n\n\n\n\n\n\n\n\n8 Q8. Factor-level ordering and Visualization\n\nInstall “gapminder” package if you don’t have it already.\n\n\nQ8.1: (1) From the package, use “gapminder” dataset. Examine the dataset to know it first. (1) First, print out the level of the “continent” factor from the gapminder data set. (2) From the most recent year in the dataset, calculate the total population by the continents and print out the outcome. (3) Print out the continent and total population again by arranging the appearance of the continent in descending order of the size of the population. (4) Compare both tables produced in (2) and (3) and describe how the order is different. Note that you haven’t changed the factor level yet. Also, state how you could change the level of the factor by the descending order of the population size. (5) change the order of the level of the “continent” factor by the size of the continent’s total population and save the change made to the continent by naming the data “gapminder1”. Make sure you confirm the operation was successful.\n\n\n```{r}\nlibrary(gapminder)\n\nstr(gapminder)\nhead(gapminder)\n\nlevels(gapminder$continent)\n\nmax_year &lt;- max(gapminder$year)\n\ncontinent_population &lt;- gapminder |&gt;\n  filter(year == max_year) |&gt;\n  group_by(continent) |&gt;\n  summarize(total_population = sum(pop))\n\nprint(continent_population)\n\ncontinent_population_sorted &lt;- continent_population |&gt;\n  arrange(desc(total_population))\n\nprint(continent_population_sorted)\n\n\ngapminder1 &lt;- gapminder |&gt;\n  mutate(continent = factor(continent, levels = continent_population_sorted$continent))\n\n# Verify the factor levels have been updated\nlevels(gapminder1$continent)\n```\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n\n\n  \n\n\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n# A tibble: 5 × 2\n  continent total_population\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Africa           929539692\n2 Americas         898871184\n3 Asia            3811953827\n4 Europe           586098529\n5 Oceania           24549947\n# A tibble: 5 × 2\n  continent total_population\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Asia            3811953827\n2 Africa           929539692\n3 Americas         898871184\n4 Europe           586098529\n5 Oceania           24549947\n[1] \"Asia\"     \"Africa\"   \"Americas\" \"Europe\"   \"Oceania\" \n\n\n\nQ8.2: (1) Using the original gapminder data (not gapminder1), draw a chart that shows the total population by each continent in the year 2007, ordered by the size of the population in the chart. As usual, do this in one chain of codes and provide the necessary information and beautification to the chart to portray accurate communication to the viewers.\n\n\n```{r}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create the chart\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  group_by(continent) %&gt;%\n  summarize(total_population = sum(pop)) %&gt;%\n  ggplot(aes(x = reorder(continent, -total_population), y = total_population, fill = continent)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Total Population by Continent (2007)\", \n       x = \"Continent\", \n       y = \"Total Population\", \n       caption = \"Data Source: Gapminder\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") +  # Nice color theme\n  theme(legend.position = \"none\", \n        text = element_text(size = 12), \n        plot.title = element_text(hjust = 0.5, face = \"bold\"))\n```\n\n\n\n\n\n\n\n\nHints: You don’t need to confirm the success of the intended wrangling in a table format since you can confirm it in the chart. This way, you can avoid breaking the chain to confirm the level.\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nQ8.1 Steps:\nChecked dataset structure (glimpse).\nPrinted factor levels of “continent.”\nFiltered data for the most recent year (2007) and calculated the total population per continent.\nArranged the results in descending order and compared them.\nReordered the factor levels based on population size and stored it in gapminder1.\nQ8.2 Visualization:\nThe bar chart displays total population by continent in 2007.\nThe fct_reorder() function ensures the x-axis is ordered by population size.\nUsed geom_col() for bars, coord_flip() for better readability, and theme_minimal() for a clean look.\n\n\n\n\n9 Q9. Shortening Factor Levels: top or bottom 5 lists\nThe goal is to draw a chart that shows the top and bottom Asian countries by life expectancy in 2007. To do so, do the following:\n\nQ9.1: let’s first filter the unaltered gapminder dataset by the year 2007 and Asia continent and see how many countries there are in the filtered data. Do this using the piping operator all the time.\n\n\n```{r}\nlibrary(gapminder)\nlibrary(dplyr)\n\ngapminder %&gt;%\n  filter(year == 2007, continent == \"Asia\") %&gt;%\n  summarise(num_countries = n())\n\n#gapminder dataset contains the country-level data on life expectancy, GDP per capitia, and population in five-year interval. We are filtering for continent = Asia and year 2007. \n```\n\n\n  \n\n\n\n\nQ9.2: How many Asian countries do you see? Given\n\n33\n\nQ9.3: Given the number of countries, it seems like a good idea to present the top 5 and bottom 5 countries, one at a time, and order the countries by life expectancy. Let’s first focus on the top 5 list. Do appropriate wrangling and visualization for the top 5 list.\n\nHints: Use functions from dplyr package to deal with top and bottom rankings. The function that will be most effective in finding the top or bottom five list would be slice_max() and slice_min(), respectively.\nTop 5 Asian Countries by Life Expectancy in 2007\n\n```{r}\nlibrary(gapminder)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntop_5_asia &lt;- gapminder %&gt;%\n  filter(continent == \"Asia\", year == 2007) %&gt;%\n  slice_max(lifeExp, n = 5)\n\n#Slice_max(lifeExp, n = 5) extracts the top 5 contries by life expectancy \n\nggplot(top_5_asia, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Top 5 Asian Countries by Life Expectancy in 2007\",\n       x = \"Country\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n```\n\n\n\n\n\n\n\nBottom 5 Asian Countries by Life Expectancy in 2007\n\n```{r}\nbottom_5_asia &lt;- gapminder %&gt;%\n  filter(continent == \"Asia\", year == 2007) %&gt;%\n  slice_min(lifeExp, n = 5)\n\n#Slice_min(lifeExp, n = 5) extracts the bottom 5 contries by life expectancy\n\nggplot(bottom_5_asia, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Bottom 5 Asian Countries by Life Expectancy in 2007\",\n       x = \"Country\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n\nExplanation of the Code slice_max(lifeExp, n = 5) → Extracts the top 5 countries by life expectancy.\nslice_min(lifeExp, n = 5) → Extracts the bottom 5 countries by life expectancy.\nreorder(country, lifeExp) → Ensures countries are sorted in the bar chart.\ngeom_col() → Creates a bar chart.\ncoord_flip() → Flips the bars for better readability.\nMinimal theme (theme_minimal()) → Improves aesthetics.\n\n\n\n\nQ9.4: Do similar wrangling and visualization to show the bottom 5 countries. (5) This time, let’s put the two charts together and show them in one pallet by using the patchwork package &lt;https://patchwork.data-imaginist.com/Links to an external site.&gt;. The end result should look like this.\n\n\n```{r}\nlibrary(patchwork)\n\n\ntop_5_asia &lt;- gapminder %&gt;%\n  filter(continent == \"Asia\", year == 2007) %&gt;%\n  slice_max(lifeExp, n = 5)\n\n\nbottom_5_asia &lt;- gapminder %&gt;%\n  filter(continent == \"Asia\", year == 2007) %&gt;%\n  slice_min(lifeExp, n = 5)\n\n\nplot_top5 &lt;- ggplot(top_5_asia, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Top 5 Asian Countries\",\n       x = \"Country\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n\n#Assign the top 5 plot a title \"plot_top5\"\n\nplot_bottom5 &lt;- ggplot(bottom_5_asia, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Bottom 5 Asian Countries\",\n       x = \"Country\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n\n#Assign the bottom 5 plot a title \"plot_bottom5\"\n\nplot_top5 | plot_bottom5\n\n#patchwork could patch the plots either vertically \"/\" or horizontally \"|\"\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow This Works\n\n\n\n\n\nCreates two ggplots (plot_top5 and plot_bottom5).\nUses patchwork package (plot_top5 / plot_bottom5) to stack them vertically.\nBoth plots share the same styling, making it easier to compare.\n\n\n\n\n10 Q10: Date-time data type (2 pts)\nThe goal is to deal with date-time data and visualize the data. The data is from the “flights” dataset that comes with the nycflights13 package. If you don’t have it install the package and load up the data.\n\nQ10.1. Look up the help to understand the data and also run appropriate functions to understand the data.\n\n\n```{r}\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nlibrary(dplyr)\n```\n\nCheck the dataset structure: this should give you the overview of the dataset like variable names and data types\n\n```{r}\nstr(flights)\n```\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\nVisulize the first few rows so that you are able to see the sample data\n\n```{r}\nhead(flights)\n```\n\n\n  \n\n\n\nWhen checking the column names you would be able to see all the variables in the dataset.\n\n```{r}\ncolnames(flights)\n```\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\n\nCheck the count of the rows and columns in the current dataset\n\n```{r}\ndim(flights)\n```\n\n[1] 336776     19\n\n\n\n```{r}\nsummary(flights)\n```\n\n      year          month             day           dep_time    sched_dep_time\n Min.   :2013   Min.   : 1.000   Min.   : 1.00   Min.   :   1   Min.   : 106  \n 1st Qu.:2013   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 907   1st Qu.: 906  \n Median :2013   Median : 7.000   Median :16.00   Median :1401   Median :1359  \n Mean   :2013   Mean   : 6.549   Mean   :15.71   Mean   :1349   Mean   :1344  \n 3rd Qu.:2013   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:1744   3rd Qu.:1729  \n Max.   :2013   Max.   :12.000   Max.   :31.00   Max.   :2400   Max.   :2359  \n                                                 NA's   :8255                 \n   dep_delay          arr_time    sched_arr_time   arr_delay       \n Min.   : -43.00   Min.   :   1   Min.   :   1   Min.   : -86.000  \n 1st Qu.:  -5.00   1st Qu.:1104   1st Qu.:1124   1st Qu.: -17.000  \n Median :  -2.00   Median :1535   Median :1556   Median :  -5.000  \n Mean   :  12.64   Mean   :1502   Mean   :1536   Mean   :   6.895  \n 3rd Qu.:  11.00   3rd Qu.:1940   3rd Qu.:1945   3rd Qu.:  14.000  \n Max.   :1301.00   Max.   :2400   Max.   :2359   Max.   :1272.000  \n NA's   :8255      NA's   :8713                  NA's   :9430      \n   carrier              flight       tailnum             origin         \n Length:336776      Min.   :   1   Length:336776      Length:336776     \n Class :character   1st Qu.: 553   Class :character   Class :character  \n Mode  :character   Median :1496   Mode  :character   Mode  :character  \n                    Mean   :1972                                        \n                    3rd Qu.:3465                                        \n                    Max.   :8500                                        \n                                                                        \n     dest              air_time        distance         hour      \n Length:336776      Min.   : 20.0   Min.   :  17   Min.   : 1.00  \n Class :character   1st Qu.: 82.0   1st Qu.: 502   1st Qu.: 9.00  \n Mode  :character   Median :129.0   Median : 872   Median :13.00  \n                    Mean   :150.7   Mean   :1040   Mean   :13.18  \n                    3rd Qu.:192.0   3rd Qu.:1389   3rd Qu.:17.00  \n                    Max.   :695.0   Max.   :4983   Max.   :23.00  \n                    NA's   :9430                                  \n     minute        time_hour                     \n Min.   : 0.00   Min.   :2013-01-01 05:00:00.00  \n 1st Qu.: 8.00   1st Qu.:2013-04-04 13:00:00.00  \n Median :29.00   Median :2013-07-03 10:00:00.00  \n Mean   :26.23   Mean   :2013-07-03 05:22:54.64  \n 3rd Qu.:44.00   3rd Qu.:2013-10-01 07:00:00.00  \n Max.   :59.00   Max.   :2013-12-31 23:00:00.00  \n                                                 \n\n\n\nQ10.2. Include only the records that do not have missing values in the following three variables: dep_time arr_time and air_time. Convert “dep_time” and “arr_time” to date-time data type, using “year”, “month”, and “day” as well as the hour and minutes information from “dep_time” and “arr_time”. Then select only some useful columns we will need to draw a chart since there are too many columns to display. Those variables are “origin”, “dest”, “air_time”, and any variables that start with ‘dep’ or ‘arr.’ Let’s save this subset of the data as “flights_lub.”\n\n\n```{r}\nlibrary(lubridate)\n\nflights_lub &lt;- flights %&gt;%\n  filter(!is.na(dep_time), !is.na(arr_time), !is.na(air_time))\n\n#Filter Records with Non-Missing Values in dep_time, arr_time, and air_time\n\nflights_lub &lt;- flights_lub %&gt;%\n  mutate(\n    dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),\n    arr_time = make_datetime(year, month, day, arr_time %/% 100, arr_time %% 100)\n  )\n\n#Convert dep_time and arr_time to Date-Time Data Type. We use the make_datetime() function from the lubridate package to combine year, month, day, and the extracted hours and minutes for both dep_time and arr_time.  \n\nflights_lub &lt;- flights_lub %&gt;%\n  select(origin, dest, air_time, starts_with(\"dep\"), starts_with(\"arr\"))\n\nhead(flights_lub)\n```\n\n\n  \n\n\n\n\n\n\n\n\n\nSummary of Key Steps\n\n\n\n\n\nFilter rows with non-missing values in dep_time, arr_time, and air_time.\nConvert dep_time and arr_time from numeric values to proper date-time format.\nSelect relevant columns for visualization.\nThe final flights_lub dataset contains the necessary columns for plotting, ready for visualization or further analysis!\n\n\n\n\nQ10.3. Using the “dep_time” variable, display the frequency distribution of the count of flights departing from New York City over the entire year in 2013, from Jan. 1 to Dec. 31.\n\n\n```{r}\nflights_lub %&gt;%\n  mutate(dep_date = as.Date(dep_time)) %&gt;%  \n  group_by(dep_date) %&gt;%  \n  summarise(flight_count = n()) %&gt;%  \n  ggplot(aes(x = dep_date, y = flight_count)) +\n  geom_line() + \n  labs(\n    title = \"Frequency Distribution of Flights Departing from NYC in 2013\",\n    x = \"Date\",\n    y = \"Number of Flights\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))  \n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of Code\n\n\n\n\n\nmutate(dep_date = as.Date(dep_time)): Extract the date part from the dep_time variable to group the flights by date.\ngroup_by(dep_date): Group the dataset by the extracted dep_date.\nsummarise(flight_count = n()): Count the number of flights for each day.\nggplot(aes(x = dep_date, y = flight_count)): Create a line plot with the date on the x-axis and the count of flights on the y-axis.\ngeom_line(): Use a line plot to visualize the frequency distribution.\nlabs(): Add a title and labels to the x and y axes.\ntheme(axis.text.x = element_text(angle = 90, hjust = 1)): Rotate the x-axis labels to improve readability, especially for the date labels.\n\n\n\n\nQ10.4. This time, let’s zoom in on just one day, the Fourth of July holiday. Display the frequency distribution of the flights departing from New York City during the entire day.\n\n\n```{r}\nflights_lub %&gt;%\n  filter(as.Date(dep_time) == \"2013-07-04\") %&gt;% \n  mutate(dep_hour = format(dep_time, \"%H:%M\")) %&gt;% \n  group_by(dep_hour) %&gt;%  \n  summarise(flight_count = n()) %&gt;%  \n  ggplot(aes(x = dep_hour, y = flight_count)) +\n  geom_bar(stat = \"identity\") +  \n  labs(\n    title = \"Frequency Distribution of Flights Departing from NYC on July 4, 2013\",\n    x = \"Time of Day\",\n    y = \"Number of Flights\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))  \n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of Code\n\n\n\n\n\nfilter(as.Date(dep_time) == “2013-07-04”): Filter the dataset for flights that departed on July 4th, 2013.\nmutate(dep_hour = format(dep_time, “%H:%M”)): Extract the hour and minute part from the dep_time variable to group the flights by time of day.\ngroup_by(dep_hour): Group the data by the extracted hour and minute.\nsummarise(flight_count = n()): Count the number of flights per time period (hour and minute).\nggplot(aes(x = dep_hour, y = flight_count)): Create a bar plot with the departure time on the x-axis and the count of flights on the y-axis.\ngeom_bar(stat = “identity”): Use a bar plot to display the frequency distribution.\nlabs(): Add a title and axis labels.\ntheme(axis.text.x = element_text(angle = 90, hjust = 1)): Rotate the x-axis labels for better readability, especially for the time of day.\n\n\n\n\nQ10.5. Using the “flights_lub” dataset, create the “travel_time” variable by subtracting “dep_time” from “arr_time”. Since arrival occurs later than departure, arr_time minus dep_time should be positive. Check this logic by sorting the data by the ascending order of “travel_time” Was the expectation met? No, because you will see negative minutes of travel_time. Pay attention to the type of data for travel_time. Why were there negative minutes?\n\n\n```{r}\nflights_lub &lt;- flights_lub %&gt;%\n  mutate(travel_time = as.numeric(difftime(arr_time, dep_time, units = \"mins\"))) \n\n\nflights_lub_sorted &lt;- flights_lub %&gt;%\n  arrange(travel_time)\n\n\nhead(flights_lub_sorted)\n```\n\n\n  \n\n\n\n\n\n\n\n\n\nExplanation of Code\n\n\n\n\n\nmutate(travel_time = as.numeric(difftime(arr_time, dep_time, units = “mins”))): This calculates the time difference between the arr_time and dep_time in minutes. The difftime() function returns the difference as a difftime object, which we then convert to numeric minutes using as.numeric().\narrange(travel_time): This sorts the data by the travel_time in ascending order, so we can check if there are any negative values (which would indicate issues).\n\n\n\n\nQ10.6. That was because those flights were overnight flights. The original dep_time had only hour and minutes information before day information was added and the time of arrival is smaller than the time of departure for the overnight flights. Thus, we need to fix “arr_time” by adding one more day when there was such an overnight flight. Do the appropriate adjustment prior to the travel_time variable creation in the chain of codes connected through a series of pipe operators. Also, convert the travel_time to a numeric data type. Show that there are no negative minutes anymore.\n\n\n```{r}\nflights_lub &lt;- flights_lub %&gt;%\n  mutate(\n    dep_time = with_tz(dep_time, tzone = \"America/New_York\"), \n    arr_time = with_tz(arr_time, tzone = \"America/New_York\")  \n  )\n\n\nflights_lub &lt;- flights_lub %&gt;%\n  mutate(travel_time = as.numeric(difftime(arr_time, dep_time, units = \"mins\")))\n\n\nnegative_travel_time &lt;- flights_lub %&gt;%\n  filter(travel_time &lt; 0)\n\n\nhead(negative_travel_time)\n\n\nflights_lub_cleaned &lt;- flights_lub %&gt;%\n  filter(travel_time &gt;= 0)\n\n\nhead(flights_lub_cleaned)\n```\n\n\n  \n\n\n  \n\n\n\nIf the travel times are negative, it’s a sign that the data might have issues, such as time zone mismatches or incorrect time entries. After identifying such cases, they can be corrected or removed from the dataset.\n\nQ10.7. Once again, zoom in on the Forth of July. How many flights departed from New York City on that day? Create a plot that shows the relationship between the “travel_time” and “air_time” What do you infer from it? Why is there not a perfect association?\n\n\n```{r}\nflights_lub_cleaned &lt;- flights_lub_cleaned %&gt;%\n  mutate(\n    year = lubridate::year(dep_time),\n    month = lubridate::month(dep_time),\n    day = lubridate::day(dep_time)\n  )\n\n# Filter the data for July 4th, 2013\nflights_july4 &lt;- flights_lub_cleaned %&gt;%\n  filter(year == 2013, month == 7, day == 4)\n\n# How many flights departed on July 4th, 2013?\nnum_flights_july4 &lt;- nrow(flights_july4)\nnum_flights_july4\n\n# Create a plot to show the relationship between travel_time and air_time\nggplot(flights_july4, aes(x = air_time, y = travel_time)) +\n  geom_point() +\n  labs(\n    title = \"Travel Time vs Air Time on July 4th, 2013\",\n    x = \"Air Time (minutes)\",\n    y = \"Travel Time (minutes)\"\n  ) +\n  theme_minimal()\n```\n\n[1] 717\n\n\n\n\n\n\n\n\nExpected Association: We might expect a positive correlation between air_time and travel_time because longer flights (in terms of air time) should generally take more time to reach their destination. However, travel_time includes not only the air time but also time spent taxiing, delays, and other factors that might increase the overall travel time.\nWhy No Perfect Association?:\nTaxing Time: A plane could have a very long taxiing time on the runway or waiting for clearance to depart or land, affecting travel_time but not air_time.\nDelays: Delays due to weather, air traffic control, or other factors might add to travel_time without affecting air_time.\nAircraft Routing: Sometimes, flights take longer routes or encounter air traffic, which would increase the travel_time but not necessarily the air_time.\nThis will show that while there is some correlation, it is not a perfect one due to other external factors impacting travel time."
  }
]